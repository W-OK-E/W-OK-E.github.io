<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://w-ok-e.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://w-ok-e.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-10T14:25:37+00:00</updated><id>https://w-ok-e.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Lights❌ Language✅ Camera…Action</title><link href="https://w-ok-e.github.io/blog/2025/quarv/" rel="alternate" type="text/html" title="Lights❌ Language✅ Camera…Action"/><published>2025-08-24T18:00:00+00:00</published><updated>2025-08-24T18:00:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/quarv</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/quarv/"><![CDATA[<p>If you are here from the last one, you would be reeling from all the researchy stuff we navigated or rather skimmed through back there. But Hey, you never really get to understand ideas completely unless you do a deep dive. And that is what we are gonna do here, a full in-depth review of the Quar-VLA papers which we discussed in the last one. So welcome to another article where you lay back with your popcorn, and let me do the heavy lifting of breaking down the complex jargon for you.</p> <h2 id="well-its-not-that-dumb-to-be-honest">Well, it’s not that dumb to be honest</h2> <p>If you noticed one major criticism of the papers discussed in the last blog were that they were mapping high level commands to a bunch of small tasks and not really into low-level motor or sensor commands. This sort of leaves the headache of infering the low level actions from the high level commands to the action manager, which then also needs to be trained as to what exactly particular “words” or “commands” map to in the action space. This is partly addressed by the Quar-VLA paper, which deals directly in the Vision-Language-Action Space, and they have also developed a specific VLA model for the same along with a multi-task dataset. But first let us explore their methodology and approach in depth, shall we?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/dawg1-480.webp 480w,/assets/img/post_ims/dawg/dawg1-800.webp 800w,/assets/img/post_ims/dawg/dawg1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/dawg1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://medium.com/data-science/a-comprehensive-overview-of-gaussian-splatting-e7d570081362">src1</a></p> <h3 id="vision-language-action-models">Vision Language Action Models</h3> <p>An important thing to undestand here that if we are planning to build a model capable of producing motor-level commands from Vision and Language, we also need to look after the frequency at which the commands need to be sent. Because in the case of a quadruped, the actions that we output can neither be too simplistic like the velocity commands given by the nav planners, or require a very high frequency similar to the motor level commands. Hence having a dataset that bridges this gap is also of prime importance and this is one of the things the authors have aimed at, and QUAR-VLA seems to be the first architecture that integrates Vision and Lanuage together to generate actions.</p> <p>To this end, we come to talk about <strong>Vision Language Action Models</strong> that integrate the visual input from the various sensors on the Quadruped along with the language commands. The end goal more or less is to train a conditional policy QUART that can interporet RGB Images, and high level commands. This policy takes RGB Images and the instructions as input and produces actions as output. Now in their paper specifically the authors aimed at a 11-dimensional space, and each action command looks something like:</p> \[\{ v_x, v_y, ω_z, θ_1, θ_2, θ_3, f, h_z, φ, s_y, h_z^{f} \}\] <p>Here, v<sub>x</sub>, v<sub>x</sub>, and ω<sub>x</sub> represent the velocities along the x-axis, y-axis, and z-axis respectively. θ<sub>x</sub>, θ<sub>x</sub>, and θ<sub>x</sub> indicate the gait pattern, f denotes the frequency, h<sub>x</sub> represents the height of the robot, φ denotes the pitch angle, s<sub>x</sub> corresponds to the foot width, h<sub>x</sub><sup>f</sup> represents the foot height, and t indicates the termination signal of the action.</p> <h4 id="generating-a-discrete-action-space">Generating a discrete action space</h4> <p>So an action policy is basically supposed to take in the images and the language input data and generate a bunch of action commands in the 11 dimensional action space. Now for each possible action(velocity, rotaion angle etc.) there is a continous range of values with a lower and upper bound which makes it very hard for the policy to reliably learn a set of actions given a particular input. Now this does not mean that we keep a set of discrete values, or else that will defeat the purporse of having a smooth action space as output. Discretization here means obtaining a set of bins for each possible action so that we can reliably say that okay “Velocity 5 m/s forward with rotation 30 degrees falls into this bin, choose a continous value from that. Effectively narrowing down the search space and “discretizing” our model’s outputs.</p> <ul> <li>Each action has a continous domain with a lower and a upper limit, this interval is divided into 256 bins of equal width = (range)/256</li> <li>And for any give target value, the corresponding index of the bin that it falls into is given by GIF(a - lower bound of the action space)/width of bins</li> </ul> <h2 id="do-as-i-can-not-as-i-say">Do As I Can, not As I say…</h2> <p>Well at first, if you thought that I was trying to be funny with the above heading, I WASN’T, it’s an actual title to an actual paper by people at Google and Everyday Robotics. Computer Science researchers seem to be the most creative people when it comes to naming their works, well anyways what bells does the title ring when you think of it in terms of LLMs and Robotics? Okay I’ll let a picture from the paper explain it to you:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/can_say1-480.webp 480w,/assets/img/post_ims/dawg/can_say1-800.webp 800w,/assets/img/post_ims/dawg/can_say1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/can_say1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://arxiv.org/abs/2204.01691">source</a></p> <p>Now imagine if instead of giving instructions to the Robotic arm, the LLM was instructing you(Arghhh they’ll rule us) despite the order in which the LLM is giving the instructions or the content of these instructions, we have a natural intuition that tells us things such as the sponge has to be picked before going to the trash can, or that maybe the cup is still there, I need to remove it before cleaning the area. Therefore we are carrying the instructions forward as “we can” not as “it was told” to us. This highlights a crucial aspect that no matter how smart LLMs are, their responses, and they themselves are not grounded in the real world, solely because they have not interacted with the actual environment.</p> <p>This is what the SayCan architecture proposed in the paper addresses, as depicted in the picture above, and they do this by treating the feedback from the robot as tokens, essentially making the robot the limbs of the LLM that allows it to interact and gain feedback from the surrounding world.</p> <h4 id="work-encompasses">Work encompasses:</h4> <ol> <li>Leveraging the rich semantic knowledge in LLMs to complete real world tasks.</li> <li>It does so by treating all the possible things that the robot can do as “skills” for e.g. lifting an item, navigating to a location are all “skills” in this context.</li> <li>For any scenario that the architecture needs to plan for, each skill is assigned a certain “value” as to how appropriate that skill would be in that scenario. Simply imagine trying to develop a website, out of all the dev skills in your bag, certain skills like react, DBMS would be handy here and hence would be assigned higher values by the system.</li> <li>Each skill is given a text-label so that it can be easily parsed by the Language model and then logically arranged to form a cohesive response that is well grounded in environmental constraints and achievable.</li> </ol> <h4 id="research-gaps">Research Gaps:</h4> <ol> <li>Since the paper is quite old, and the LLMs were relatively small(in terms of parameter count) their abilities were limited by the training data.</li> <li>The range of skills(for instance a simple robotic arm which is fixed, can be dextrous but fail to follow navitation commands) that the agent has also poses a bottleneck.</li> <li>The paper does not explore robot planning and language, and other ways of combining language with robotic control.</li> </ol> <h2 id="can-you-just-break-it-down-for-me">Can you just break it down for me?</h2> <p>Well the above heading is my goto prompt these days to those Chatbots whenever I feel stuck in a mess of jargon and buzzwords. Oh wait! That makes me think, what if all our instructions are nothing but huge and confusing web of jargon and buzzwords to an agent. For instance when I ask Sam to “walk over to the guy wearing the red shirt with this doc”, could it be that it gets lost in the multitudes of possible actions that it can take but can’t decide for sure just because the action is not realy clear to it. That is where the paper : “Language Models as Zero Shot Planners” comes in, it leverages LLMs to break down high-level tasks into a bunch of smaller and actionable steps(i.e. actions closer to “skills” as described in SayCan paper).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/zero_shot1-480.webp 480w,/assets/img/post_ims/dawg/zero_shot1-800.webp 800w,/assets/img/post_ims/dawg/zero_shot1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/zero_shot1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.iconfinder.com/icons/10874016/robot_solution_robot_confusion_robot_problem_confused_person_worried_person_icon">source</a></p> <h4 id="work-encompasses-1">Work encompasses:</h4> <ol> <li>Investigates the possibility of breaking down high level tasks into smaller actionable steps.</li> <li>Instead of learning the mapping via step-by-step actions on “how to act”, it relies on the semantic information inside LLMs to extract that mapping.</li> <li>But don’t be misled,even though they donot rely on command-action pairs to learn the mappings, the LLMs are shown the mapping between textual representations of high-level and low-level commands. It’s like telling a child: “Go Play football” means “Put your shoes on, step out of the house, walk towards the field, play.” (Gosh kids these days….)</li> <li>This preconditioning improves the performance over LLM baselines.</li> </ol> <h4 id="research-gaps-1">Research Gaps:</h4> <ol> <li>Even though the LLMs can output logically correct commands, the absence of feedback from the environment reduces chances of smaller actions being actually executable. For instance, for the high level command: “Clean the room” -&gt; “grab the vacuum cleaner” might not be an executable step because the environment does not have one.</li> <li>Reliance on the belief that the simpler tasks that are generated by the LLM(i.e. mid-level actions) are already doable by the lower-level sensorimotor actions and that these lower-level action donot need any instructions from the LLM.</li> </ol> <h2 id="the-power-of-the-sun-in-the-palm-e-of-my-hand">The power of the sun, in the PaLM-E of my hand…</h2> <p>Well, if you recognized the above dialogue to be from Spider Man, kudos buddy both of us are spidey lovers! But instead if you recognized google’s LLM PaLM in there, hats off <em>~ ~</em>. See, one thing is sort of obvious now, that LLMs regardless of how good they are, struggle with acting under the constraints of the actual environment i.e. they are not grounded. Imagine them being like a super smart frog in the well who knows everything but struggles to make completely accurate decisions about being in a savannah simply because he has never been there. And if you want to understand the work done by the authors of PaLM-E you can imagine giving the frog extended sensors which sort of tell him the conditions in the Savannah. Their LLM takes sensor data, the state of the robot and textual input all as “sentences” ofc. with relevant encoding. And it made some decent leaps, as in being able to do sequential planning from textual input, however the wise reader would look behind the glam and see that being from google, the model is HUGEEEE, I mean 562B parameter. So it’s like sort of saying GPT 3.5 was better than GPT 3 because it had more parameters. Ofcourse that is not the case entirely here, as they way the model is trained also matters but it’d be interesting to see if lighter adaptations incorporating multiple sensors and sources exist. However some people may think about the parameter count of the chatbots we’ve come to know and love, and question whether 562B is really that huge…</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/llm_size.webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/llm_size.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.reddit.com/r/Infographics/comments/1c81n2o/the_size_of_llms_apr_2024/">source</a></p> <h4 id="work-encompasses-2">Work encompasses:</h4> <ol> <li>Injects continous sensory data from the robot model into the LLM, meaning the embedding space is multi-modal(basically saying data of different types is encoded to live in the same space where it can be used by the LLM)</li> <li>Capable of performing sequential manipulation task and does not need another LLM to break down the task into simpler ones. For instance asked to “Make a Cake Batter from all the ingredients you see”, it successfully decodes the sub-steps involved in the process like (Crack egg, put egg in bowl..etc.)</li> <li>Similar to the past papers, relies on the fact that the robot is well-versed in low-level policies i.e. the LLM won’t break down the action of “Crack egg” further into joint or motor commands, the “skills” of basic movement and handling are already learnt by the agent, and PaLM-E is only concerned with providing the high level commands in an autoregressive(one token at a time based on the past sequence) manner.</li> <li>Training on large scale vision language datasets, allows the model to be highly accurate in embodied tasks i.e. tasks which require awareness of your surroundings given that you are able to see if not feel it.</li> </ol> <h4 id="research-gaps-2">Research Gaps:</h4> <ol> <li>Parameter size is a big red flag that throws any question of practical deployement out the window.</li> <li>Again, similar to previous papers, reliance on the belief that the simpler tasks that are generated by the LLM(i.e. mid-level actions) are already doable by the lower-level sensorimotor actions and that these lower-level action donot need any instructions from the LLM.</li> <li>Performance with noisy sensor data across different environmental settings needs deeper evaluation.</li> </ol> <h2 id="what-if-you-gave-me-everything-in-one-place">What if you gave me everything in one place?</h2> <p>Well if you noticed something in the papers so far is that all of them generate a bunch of instructions for the agent to follow from the prompt that is given to them. So the structure looks something like a language command, which the LLM parses to generate a symbolc plan and then generate certain mid-level commands that the agent can follow based on the ‘skills’ or low-level sensorimotor movements. There seems to be a slight disconnect between the prompt to the LLM and the final actions performed by the agent, and you rely on the robot’s ability to correctly break down the tasks into the right set of low-level commands. This gap is bridged by Quar-VLA, which treates everything from the initial command to the final actions that the agent recieves as a single pipeline.</p> <p>So it is somewhat similar to the model actually providing explicit commands to he agent to carry out the actions that would help it achieve the high level semantic task. This can look like “Rotate Hip Joint by 30 degrees”, or “actuate the knee joint” etc.</p> <h4 id="work-encompasses-3">Work Encompasses</h4> <ol> <li>Proposed a new paradigm that integrates the vision input from the Quadruped’s sensor, the language ingretation and action generation.</li> <li>The interesting factor here was that this was not just limited to carrying out tasks, but also extended to navigation and manipulation which required much complex understanding of the environment. Like for instance if I want Sam to navigate in a room towards a red target, it must first be able to detect the target and move towards it.</li> <li>The also compiled all of the sensor data collected into a single database(QUARD) which can be utilised for end to end training for other Robotic agents.</li> </ol> <h4 id="research-gaps-3">Research Gaps</h4> <ol> <li>The environment can be improved - more terrrains, with different configurations.</li> <li>They don’t seem to have tested different path planning algorithms extensively, and for control they are only using a PD Controller, maybe we can check on the limitations for that, and improve it.</li> <li>Also, while they have shown extensive results in simulation, the results for actual environment might need further evaluations.</li> <li>Latency remains an issue, the response time can be improved.</li> </ol> <h2 id="our-tea-for-the-second-timepicked-by-a-bot">Our Tea for the second time…picked by a bot!</h2> <p>The RT-2 paper does something extra, in the sense the techniques adopted by the users developed good enough policies to allow the Quadrupeds to perform “unseen” tasks. The key to this was the training data employed by the authors. They not only trained on robot demonstration data(i.e. command-action pairs) but also extensive data from the web, which probably explains their ability to be able to carry out the unseen tasks. This also leverages the transformer architecture, which means it is a bit memory intensive, now if you are someone who is not familiar with the transformer architecture, hang tight, an explanation for that will be up soon here :) The usage of the transformer architecture means that the inputs and the outputs are handled as tokens(Just like how GPT or your fav LLM writes in “bits” sequentially, that bit-styled writing is a result tokenization)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/quad_in_sim-480.webp 480w,/assets/img/post_ims/dawg/quad_in_sim-800.webp 800w,/assets/img/post_ims/dawg/quad_in_sim-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/quad_in_sim.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="work-encompasses-4">Work Encompasses</h4> <ol> <li>Training on robotic demonstrations(sample robotic actions) as well as vast internet data</li> <li>From the prompt, the action commands are given out as discrete tokens(coz transformers) which means that continous actions(like rotate 30 deg while moving ahead) are given out as discrete tokens.</li> <li>Shows strong performance on semantic reasoning tasks.</li> </ol> <h4 id="research-gaps-4">Research Gaps</h4> <ol> <li>Low-level controls have not been addressed properly in this architecture as well, that is it is left to the agent to interpret mid-level commands accurately and execute the right low-level sensori-motor commands.</li> <li>Well, as awesome as the idea of training on the entire web sounds, it is a difficult mapping from the wide array of meanings one can extract from the various knowledge base of the internet versus the actual grounding that we would expect.</li> <li>Besudes, training models on such large databases requires extensive computing resources, and the need to complement simulation data.</li> </ol> <p>Pheww, that was quite a lot of unpacking, don’t you think? But what is awestriking is the speed with which progress is being made in this field, it’s almost always buzzing with some or the other activity. Now with a basic literature review underway, I’ll start setting up the environment for testing the integration of Quadrupeds out, so stick around for more updates, they are just around the corner ;)</p>]]></content><author><name></name></author><category term="Posts"/><category term="Deep"/><category term="Learning,"/><category term="Research,"/><category term="LLMs,"/><category term="Robotics,"/><category term="Quadruped,"/><category term="Vision"/><category term="Language"/><category term="Models."/><summary type="html"><![CDATA[Deep Dive into the Quar-VLA paper]]></summary></entry><entry><title type="html">Your dog can sniff, mine scans…</title><link href="https://w-ok-e.github.io/blog/2025/quad/" rel="alternate" type="text/html" title="Your dog can sniff, mine scans…"/><published>2025-08-17T18:00:00+00:00</published><updated>2025-08-17T18:00:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/quad</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/quad/"><![CDATA[<p>The title of this blog is kind of misleading, and I apologize for that. This is not about describing our Quadruped at Project MANAS a.k.a Sam, “scans” v/s “sniffs”, it is more about using that distinctive mechanical and electrical ability to improve it’s intelligence. And unless you have been living under a rock, you would know that the hottest intelligent systems in ‘da club right not are LLMs, so I thought to myself, what if we could integrate LLMs with the Quadruped and see how it improves it’s capacity to undertake various tasks.</p> <h2 id="well-its-not-that-dumb-to-be-honest">Well, it’s not that dumb to be honest</h2> <p>The Quadruped procurement for Project MANAS was actually the brainchild of our seniors and super-seniors who had interned at IISC Bangalore’s robotics laboratory. Now, being the powerhouse that IISC Bangalore is, among the various projects being worked upon, the Quadruped was one of them. So why not have our own test bench that allows us to learn about these systems, while at the same time developing and researching in an exciting domain.</p> <p>So it’s quite smart for a mechanical dog I would say, autonomous navigation is not it’s best suit I’d say, but it does manage to navigate to straight goals, it can follow certain hard-coded voice commands. But what really bugs me is that while you can ask Sam to Wiggle it’s butt and show you a dance, you can’t ask it to simply bring you a document that your friend wants you to have a look at. And I was like well, why not incorporate that system into it then? So I turned to the most intelligent(Supposedly) systems of today - LLMs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/dawg1-480.webp 480w,/assets/img/post_ims/dawg/dawg1-800.webp 800w,/assets/img/post_ims/dawg/dawg1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/dawg1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://medium.com/data-science/a-comprehensive-overview-of-gaussian-splatting-e7d570081362">src1</a></p> <h3 id="when-in-doubt-go-to-the-library-arxiv">When in doubt, go to <del>the library</del> arxiv</h3> <p>Okay so since we have just begun this venture, just like any good research project I put my head out to sniff if people have already done something similar, and viola did I find it. Every major corporation from Google to Nvidia(I was kinda sad I couldn’t read a boston dynamics paper but okay) is heavily invested in this and have been trying to incorporate LLMs into Robotic agents in order to make them smarter, in today’s blog, I will be briefly breaking down 5 such papers for you, listing out some key points along with what they achieved and what they missed out on. And ofcourse just like anything on the internet today, this list is incomplete without Google.</p> <h2 id="do-as-i-can-not-as-i-say">Do As I Can, not As I say…</h2> <p>Well at first, if you thought that I was trying to be funny with the above heading, I WASN’T, it’s an actual title to an actual paper by people at Google and Everyday Robotics. Computer Science researchers seem to be the most creative people when it comes to naming their works, well anyways what bells does the title ring when you think of it in terms of LLMs and Robotics? Okay I’ll let a picture from the paper explain it to you:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/can_say1-480.webp 480w,/assets/img/post_ims/dawg/can_say1-800.webp 800w,/assets/img/post_ims/dawg/can_say1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/can_say1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://arxiv.org/abs/2204.01691">source</a></p> <p>Now imagine if instead of giving instructions to the Robotic arm, the LLM was instructing you(Arghhh they’ll rule us) despite the order in which the LLM is giving the instructions or the content of these instructions, we have a natural intuition that tells us things such as the sponge has to be picked before going to the trash can, or that maybe the cup is still there, I need to remove it before cleaning the area. Therefore we are carrying the instructions forward as “we can” not as “it was told” to us. This highlights a crucial aspect that no matter how smart LLMs are, their responses, and they themselves are not grounded in the real world, solely because they have not interacted with the actual environment.</p> <p>This is what the SayCan architecture proposed in the paper addresses, as depicted in the picture above, and they do this by treating the feedback from the robot as tokens, essentially making the robot the limbs of the LLM that allows it to interact and gain feedback from the surrounding world.</p> <h4 id="work-encompasses">Work encompasses:</h4> <ol> <li>Leveraging the rich semantic knowledge in LLMs to complete real world tasks.</li> <li>It does so by treating all the possible things that the robot can do as “skills” for e.g. lifting an item, navigating to a location are all “skills” in this context.</li> <li>For any scenario that the architecture needs to plan for, each skill is assigned a certain “value” as to how appropriate that skill would be in that scenario. Simply imagine trying to develop a website, out of all the dev skills in your bag, certain skills like react, DBMS would be handy here and hence would be assigned higher values by the system.</li> <li>Each skill is given a text-label so that it can be easily parsed by the Language model and then logically arranged to form a cohesive response that is well grounded in environmental constraints and achievable.</li> </ol> <h4 id="research-gaps">Research Gaps:</h4> <ol> <li>Since the paper is quite old, and the LLMs were relatively small(in terms of parameter count) their abilities were limited by the training data.</li> <li>The range of skills(for instance a simple robotic arm which is fixed, can be dextrous but fail to follow navitation commands) that the agent has also poses a bottleneck.</li> <li>The paper does not explore robot planning and language, and other ways of combining language with robotic control.</li> </ol> <h2 id="can-you-just-break-it-down-for-me">Can you just break it down for me?</h2> <p>Well the above heading is my goto prompt these days to those Chatbots whenever I feel stuck in a mess of jargon and buzzwords. Oh wait! That makes me think, what if all our instructions are nothing but huge and confusing web of jargon and buzzwords to an agent. For instance when I ask Sam to “walk over to the guy wearing the red shirt with this doc”, could it be that it gets lost in the multitudes of possible actions that it can take but can’t decide for sure just because the action is not realy clear to it. That is where the paper : “Language Models as Zero Shot Planners” comes in, it leverages LLMs to break down high-level tasks into a bunch of smaller and actionable steps(i.e. actions closer to “skills” as described in SayCan paper).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/zero_shot1-480.webp 480w,/assets/img/post_ims/dawg/zero_shot1-800.webp 800w,/assets/img/post_ims/dawg/zero_shot1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/zero_shot1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.iconfinder.com/icons/10874016/robot_solution_robot_confusion_robot_problem_confused_person_worried_person_icon">source</a></p> <h4 id="work-encompasses-1">Work encompasses:</h4> <ol> <li>Investigates the possibility of breaking down high level tasks into smaller actionable steps.</li> <li>Instead of learning the mapping via step-by-step actions on “how to act”, it relies on the semantic information inside LLMs to extract that mapping.</li> <li>But don’t be misled,even though they donot rely on command-action pairs to learn the mappings, the LLMs are shown the mapping between textual representations of high-level and low-level commands. It’s like telling a child: “Go Play football” means “Put your shoes on, step out of the house, walk towards the field, play.” (Gosh kids these days….)</li> <li>This preconditioning improves the performance over LLM baselines.</li> </ol> <h4 id="research-gaps-1">Research Gaps:</h4> <ol> <li>Even though the LLMs can output logically correct commands, the absence of feedback from the environment reduces chances of smaller actions being actually executable. For instance, for the high level command: “Clean the room” -&gt; “grab the vacuum cleaner” might not be an executable step because the environment does not have one.</li> <li>Reliance on the belief that the simpler tasks that are generated by the LLM(i.e. mid-level actions) are already doable by the lower-level sensorimotor actions and that these lower-level action donot need any instructions from the LLM.</li> </ol> <h2 id="the-power-of-the-sun-in-the-palm-e-of-my-hand">The power of the sun, in the PaLM-E of my hand…</h2> <p>Well, if you recognized the above dialogue to be from Spider Man, kudos buddy both of us are spidey lovers! But instead if you recognized google’s LLM PaLM in there, hats off <em>~ ~</em>. See, one thing is sort of obvious now, that LLMs regardless of how good they are, struggle with acting under the constraints of the actual environment i.e. they are not grounded. Imagine them being like a super smart frog in the well who knows everything but struggles to make completely accurate decisions about being in a savannah simply because he has never been there. And if you want to understand the work done by the authors of PaLM-E you can imagine giving the frog extended sensors which sort of tell him the conditions in the Savannah. Their LLM takes sensor data, the state of the robot and textual input all as “sentences” ofc. with relevant encoding. And it made some decent leaps, as in being able to do sequential planning from textual input, however the wise reader would look behind the glam and see that being from google, the model is HUGEEEE, I mean 562B parameter. So it’s like sort of saying GPT 3.5 was better than GPT 3 because it had more parameters. Ofcourse that is not the case entirely here, as they way the model is trained also matters but it’d be interesting to see if lighter adaptations incorporating multiple sensors and sources exist. However some people may think about the parameter count of the chatbots we’ve come to know and love, and question whether 562B is really that huge…</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/llm_size.webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/llm_size.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.reddit.com/r/Infographics/comments/1c81n2o/the_size_of_llms_apr_2024/">source</a></p> <h4 id="work-encompasses-2">Work encompasses:</h4> <ol> <li>Injects continous sensory data from the robot model into the LLM, meaning the embedding space is multi-modal(basically saying data of different types is encoded to live in the same space where it can be used by the LLM)</li> <li>Capable of performing sequential manipulation task and does not need another LLM to break down the task into simpler ones. For instance asked to “Make a Cake Batter from all the ingredients you see”, it successfully decodes the sub-steps involved in the process like (Crack egg, put egg in bowl..etc.)</li> <li>Similar to the past papers, relies on the fact that the robot is well-versed in low-level policies i.e. the LLM won’t break down the action of “Crack egg” further into joint or motor commands, the “skills” of basic movement and handling are already learnt by the agent, and PaLM-E is only concerned with providing the high level commands in an autoregressive(one token at a time based on the past sequence) manner.</li> <li>Training on large scale vision language datasets, allows the model to be highly accurate in embodied tasks i.e. tasks which require awareness of your surroundings given that you are able to see if not feel it.</li> </ol> <h4 id="research-gaps-2">Research Gaps:</h4> <ol> <li>Parameter size is a big red flag that throws any question of practical deployement out the window.</li> <li>Again, similar to previous papers, reliance on the belief that the simpler tasks that are generated by the LLM(i.e. mid-level actions) are already doable by the lower-level sensorimotor actions and that these lower-level action donot need any instructions from the LLM.</li> <li>Performance with noisy sensor data across different environmental settings needs deeper evaluation.</li> </ol> <h2 id="what-if-you-gave-me-everything-in-one-place">What if you gave me everything in one place?</h2> <p>Well if you noticed something in the papers so far is that all of them generate a bunch of instructions for the agent to follow from the prompt that is given to them. So the structure looks something like a language command, which the LLM parses to generate a symbolc plan and then generate certain mid-level commands that the agent can follow based on the ‘skills’ or low-level sensorimotor movements. There seems to be a slight disconnect between the prompt to the LLM and the final actions performed by the agent, and you rely on the robot’s ability to correctly break down the tasks into the right set of low-level commands. This gap is bridged by Quar-VLA, which treates everything from the initial command to the final actions that the agent recieves as a single pipeline.</p> <p>So it is somewhat similar to the model actually providing explicit commands to he agent to carry out the actions that would help it achieve the high level semantic task. This can look like “Rotate Hip Joint by 30 degrees”, or “actuate the knee joint” etc.</p> <h4 id="work-encompasses-3">Work Encompasses</h4> <ol> <li>Proposed a new paradigm that integrates the vision input from the Quadruped’s sensor, the language ingretation and action generation.</li> <li>The interesting factor here was that this was not just limited to carrying out tasks, but also extended to navigation and manipulation which required much complex understanding of the environment. Like for instance if I want Sam to navigate in a room towards a red target, it must first be able to detect the target and move towards it.</li> <li>The also compiled all of the sensor data collected into a single database(QUARD) which can be utilised for end to end training for other Robotic agents.</li> </ol> <h4 id="research-gaps-3">Research Gaps</h4> <ol> <li>The environment can be improved - more terrrains, with different configurations.</li> <li>They don’t seem to have tested different path planning algorithms extensively, and for control they are only using a PD Controller, maybe we can check on the limitations for that, and improve it.</li> <li>Also, while they have shown extensive results in simulation, the results for actual environment might need further evaluations.</li> <li>Latency remains an issue, the response time can be improved.</li> </ol> <h2 id="our-tea-for-the-second-timepicked-by-a-bot">Our Tea for the second time…picked by a bot!</h2> <p>The RT-2 paper does something extra, in the sense the techniques adopted by the users developed good enough policies to allow the Quadrupeds to perform “unseen” tasks. The key to this was the training data employed by the authors. They not only trained on robot demonstration data(i.e. command-action pairs) but also extensive data from the web, which probably explains their ability to be able to carry out the unseen tasks. This also leverages the transformer architecture, which means it is a bit memory intensive, now if you are someone who is not familiar with the transformer architecture, hang tight, an explanation for that will be up soon here :) The usage of the transformer architecture means that the inputs and the outputs are handled as tokens(Just like how GPT or your fav LLM writes in “bits” sequentially, that bit-styled writing is a result tokenization)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/quad_in_sim-480.webp 480w,/assets/img/post_ims/dawg/quad_in_sim-800.webp 800w,/assets/img/post_ims/dawg/quad_in_sim-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/quad_in_sim.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="work-encompasses-4">Work Encompasses</h4> <ol> <li>Training on robotic demonstrations(sample robotic actions) as well as vast internet data</li> <li>From the prompt, the action commands are given out as discrete tokens(coz transformers) which means that continous actions(like rotate 30 deg while moving ahead) are given out as discrete tokens.</li> <li>Shows strong performance on semantic reasoning tasks.</li> </ol> <h4 id="research-gaps-4">Research Gaps</h4> <ol> <li>Low-level controls have not been addressed properly in this architecture as well, that is it is left to the agent to interpret mid-level commands accurately and execute the right low-level sensori-motor commands.</li> <li>Well, as awesome as the idea of training on the entire web sounds, it is a difficult mapping from the wide array of meanings one can extract from the various knowledge base of the internet versus the actual grounding that we would expect.</li> <li>Besudes, training models on such large databases requires extensive computing resources, and the need to complement simulation data.</li> </ol> <p>Pheww, that was quite a lot of unpacking, don’t you think? But what is awestriking is the speed with which progress is being made in this field, it’s almost always buzzing with some or the other activity. Now with a basic literature review underway, I’ll start setting up the environment for testing the integration of Quadrupeds out, so stick around for more updates, they are just around the corner ;)</p>]]></content><author><name></name></author><category term="Posts"/><category term="Deep"/><category term="Learning,"/><category term="Research,"/><category term="LLMs,"/><category term="Robotics,"/><category term="Quadruped."/><summary type="html"><![CDATA[Well, it was time I made our dog smarter, afterall the LLMs are tasked to do that aren't they?]]></summary></entry><entry><title type="html">Pictures of the Unseen…</title><link href="https://w-ok-e.github.io/blog/2025/difix/" rel="alternate" type="text/html" title="Pictures of the Unseen…"/><published>2025-08-10T18:00:00+00:00</published><updated>2025-08-10T18:00:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/difix</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/difix/"><![CDATA[<p>I have been recently pulled into the world of 3D reconstrucion while I was interning this summer at an awesome lab, and trust me the wild possibilities that exist in this space are just out of this world. I am huge fans of game engines and various high fidelity animations, but what was even more exhilirating was to be able to understand some of the basic principles and concepts behind these techniques, and then utilise them to bring 2d images to life. In this blog, we will be discussing about the various methodologies that I used for 3D reconstruction, and also an exciting aspect that we have taken up recently. So let’s get into it.</p> <h2 id="the-great-gauss-to-the-rescue">The Great Gauss to the rescue</h2> <p>What are the ways in which you can think of representing 2D images in 3D? PointClouds, or maybe structural sheets wrapped around a rough skeleton of the model. Now we will discuss pointclouds and meshes later in the blog, but first I wanted to talk about something very unique and awe-striking - <strong>Gaussian Splatting</strong>. Now I don’t about you, but having seen the number of places where the name of Carl Fredrick Gauss pops up, just blows my mind. No seriously, you should have a look at it, that dude is awesome.</p> <p>Okay let’s understand the basic idea first - basically what you have is multiple 3D blobs which are either initialized randomly or based on a sparse reconstruction of the final structure from the images we have(there is a step prior to this called SFM which I’m glossing over here, but that basically involves obtaining camera poses and matching image features from a set of given images.)</p> <p>These blobs or Gaussians(hail Gauss) too have some specific properties of “volumetric radiance fields”, which basically means that the gaussian blobs we have, are capable of modelling what colors will appear and what brightness when we look at it from certain angles for any point in 3D space(which if you know, sounds very similar to a NERF or Neural Radiance Fields concept-wise but they’re different in the way structures are represented), specifically they look something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/gs_nerf-480.webp 480w,/assets/img/post_ims/difix/gs_nerf-800.webp 800w,/assets/img/post_ims/difix/gs_nerf-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/gs_nerf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://medium.com/data-science/a-comprehensive-overview-of-gaussian-splatting-e7d570081362">src1</a></p> <p>But the real magic of Gaussian Splats is that they donot fall under the enraging umbrella of Neural Networks at runtime and hence beautifully illustrates how an algorithm that relies on classical methods and mathematics can still deliver great products. Although just to clarify, it does use backpropagation to optimize the parameters of the various gaussians that are initialized. So in brief once you have a sparse reconstruction of images, you take those 3d points as initial means for the Gaussians all of which have their own specific properties. These properties are then optimized via backpropagation.</p> <p>During rendering, we need to iterate through each and every pixel in the H x W image because each of them acted as a mean for the gaussian that was initialized and optimized. But again, it is important to note that during this rendering, this doesn’t pass through an MLP or a Neural Network unlike NERFs, which makes Gaussian Splatting a little faster during rendering.</p> <h2 id="why-make-a-mess-mesh">Why make a <del>Mess</del> Mesh?</h2> <p>Okay so it might have felt like blind worship of Gaussian splat in the last section(and it was because I am a huge fan of it) but Gaussian splatting also produces a lot of artifacts in the recosntruction. You can think of as when some blobs or gaussians are too elongted, and can appear spread over the place. Meshes on the other hand, can be imagined like a cloth wrapping around a skeleton structure and taking up the structural features of the model in question. This “cloth” is actually an oversimplification of many different elements that could be connected to each other in either regular or irregular patterns. Meshes although great at representing smooth features and edges, struggle with representing complex lightning conditions, while at the same time, even though gaussian splats can represent intricate details and various lightning conditions, they struggle with stray gaussians, especially at edges and in smooth areas. The following image really delivers the point of a mesh being a cloth stitched from various smaller subcomponents.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/cat_mesh-480.webp 480w,/assets/img/post_ims/difix/cat_mesh-800.webp 800w,/assets/img/post_ims/difix/cat_mesh-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/cat_mesh.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.artstation.com/marketplace/p/Ky67/cartoon-cat-base-mesh-3d-model">src</a></p> <h3 id="when-they-borrow-the-invisibility-cloak">When they borrow the invisibility cloak…</h3> <p>Now obviously there are many more methods for 3D reconstruction and still more variants of the same, and I could keep talking about them forever but this blog is about the missing pieces. Regardless of the methodology at play, one should not forget that 3D not just derives but relies on 2D data. If we donot have the snapshots of a particular scene/object from certain angles, it becomes very hard to model them. In case of Gaussian Splats,if we donot have ground truth captures for a model from certain angles, the mean points for gaussians in that particular region will be missing, which means that the gaussians surrounding that area will now try to fill that region up, which either means artifacts in the final reconstruction or poor splat generation.</p> <p>Here is a snapshot of a 3D reconstruction of a temple in Orissa - Somnath, which was done using fewer images than required:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/splat_decent-480.webp 480w,/assets/img/post_ims/difix/splat_decent-800.webp 800w,/assets/img/post_ims/difix/splat_decent-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/splat_decent.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>[source - Image from Author]</p> <h3 id="diffusing-it-away">Diffusing it away</h3> <p>Okay so let us first round out the problem that we have here - We are missing snapshots from certain angles or poses, and hence the view generated from is full of artifacts. What if, there was a way to take a snapshot of that view with artifacts and also obtain the camera pose along with it, because remember - for most 3D reconstrucion pipelines, we don’t just need the images, we also need the camera poses, so we also need a way to sort of first obtain the novel camera position. The approach we are gonna follow will be something like:</p> <ul> <li>Given the training or the ground truth camera positions already present in out dataset, we can interpolate to find camera positions that are not already there.</li> <li>From the novel camera poses that we obtain, we would then need a way to sort take a snapshot to get the novel view which might contain artifacts from the surrounding gaussians</li> <li>Once we have this novel view, we need to think of a way to get rid of these artifacts, and obtaining something closer to the ground truth view.</li> </ul> <p>Now I don’t about you, but in hindsight, to me this feels like an elegant idea that sounds so trivial. This is partly inspired from a few research papers I came across and also inputs from my professor and mentors.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/camera_poses-480.webp 480w,/assets/img/post_ims/difix/camera_poses-800.webp 800w,/assets/img/post_ims/difix/camera_poses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/camera_poses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Visualizing Camera Poses for a 3D model" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>[source - Image from Author]</p> <p>So having established the idea, let us see the ways in which we can attain the aforesaid goals. Now interpolating and taking snapshots can only happen once we have rendered the model as a gausian splat. And for the rendering part, we have NerfStudio, and using the python API we can even automate the process headless.</p> <p>We can interpolate from the existing camera positions and then rasterize the splats to obtain the snapshot, so that we end up with additional camera positions and their corresponding novel views, which can be used downstream for training. But there’s a catch, these images that we have obtained have artifacts so we need to find a way of fixing that.</p> <h3 id="controlled-diffusion">Controlled Diffusion</h3> <p>I am pretty sure you must have experimented with Ghibli art style when the trend was hot, but no we aren’t gonna talk about it today. We are gonna take up it’s predecessor - Stable Diffusion. Diffusion is capable of generating realistic images just from noise, essentially it learns that mapping of being able to “diffuse” noise in just the right amounts to end up with an image. But then if you have ever used any of those awesome demos online, you would have noticed that “A happy Koala made out of Blueberry Cake” doesn’t always yield the same image, there are variations to every generation, unless ofcourse we make the models deterministic by fixing the random samplers and the seed.</p> <p>And by now you must have guessed it, that if we plan to use a diffusion model to “fix” the artifacts or say “diffuse” them away predictively from our novel views, we will need some way of controlling the process. That is where another one of Deep Learning models come in - <strong>ControlNets</strong>!</p> <p><strong>ControlNets</strong> essentially aim at more informed or controlled image generation from diffusion models. Imagine your cat posed really beautifully the other day, and now you’re craving some more of the same 🐈. And ofcourse the mean creatures that they are, your cat is refusing to recreate the master-piece, add to that the utter difficulty it has been to explain the pose to a generative model. That’s where ControlNets come in handy, you can pass the image of your cat along with some other variants of it like Canny Edge, Depth etc. and then “condition” your model to then generate a cat in that particular pose.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/Control.webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/Control.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Visualizing Camera Poses for a 3D model" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://generativeai.pub/how-to-setup-controlnet-for-stable-diffusion-ai-step-by-step-guide-aafff8996719">src</a></p> <p>This miraculously allows us to condition diffusion models to “fix” the image containing artifacts by passing the nearest ground truth image, their depth maps(denoting the distance from the camera), the confidence map(what is the confidence score of each gaussian in the captured snapshot) along with the view that we need to fix, and viola, the model should fix this, and even though right now we are still working on this, I am super excited about this line of research, because just being able to spin models without even rendering them and them fixing those “broken” views via “controlled” generation just blows my mind, what about you?</p>]]></content><author><name></name></author><category term="Posts"/><category term="Diffusion,"/><category term="Deep"/><category term="Learning,"/><category term="Computer"/><category term="Vision,"/><category term="Research"/><summary type="html"><![CDATA[A short description of how we aim to improve 3D reconstruction using Diffusion Priors.]]></summary></entry><entry><title type="html">Is Cancer Modern or Retro &amp;gt;_&amp;lt; ?</title><link href="https://w-ok-e.github.io/blog/2025/cancer/" rel="alternate" type="text/html" title="Is Cancer Modern or Retro &amp;gt;_&amp;lt; ?"/><published>2025-08-03T14:24:00+00:00</published><updated>2025-08-03T14:24:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/cancer</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/cancer/"><![CDATA[<p>We have all known for quite some time now that Cancer is caused by mutations in the Human DNA which results in uncontrolled proliferation of the cell, resulting in tumors which sort of disrupt every function that the body is capable of. But believe you me, it was not always this simple. We still do not understand the exact mechanism behind this menace which is one of the reasons why it is so hard to come up with a cure. But today’s tale is not about the technicalities, rather it is about how once the research community was divided between viral and the genetic possibilities as potential causes for Cancer.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/cancer1/virus_vs_dna-480.webp 480w,/assets/img/post_ims/cancer1/virus_vs_dna-800.webp 800w,/assets/img/post_ims/cancer1/virus_vs_dna-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/cancer1/virus_vs_dna.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.dreamstime.com/illustration/dna-cartoon.html">src1</a> <a href="https://www.istockphoto.com/photos/coronavirus-animation">src2</a></p> <h2 id="but-why-look-at-viruses-at-all">But why look at Viruses at all?</h2> <p>If Alexander Fleming and other scientists had never managed to reproduce penicillin in subsequent experiments and tests once it appeared accidently on the petri dishes, society might still be plagued by even the simplest of ailments. Simply put, in order to better understand something or make use of the discovery, it must be easily reproducible in the laboratory, and in the late 1960s this was not the case with Cancer. Although the obvious culprits had been rounded out by then - Tobacco, asbestos, X-Rays, but I think that even you would agree that it is not the most scientific or the best method to induce cancer in a cell/subject by exposing it to lethal doses of X-Rays, or making a person smoke 20 cigarattes a day. They needed a reiable way of inducing caner in cells which can then be studied further.</p> <p>But there was one such candidate - <strong>Rous’s Sarcoma Virus</strong> was a rare virus that caused a rare cancer in a species of Chicken. And it’s namesake Peyton Rous, was already pushing Viruses as the key missing piece in the puzzle of Cancer’s causes. But little did he realise that this push would become the missing piece to a different puzzle than Cancer Viriology. But RSV came with it’s own caveats, the virus had never been isolated on a petri-dish to reliably study it. Until One day…</p> <h3 id="cancer-in-a-petri-dish">Cancer in a Petri-Dish:</h3> <p>Howard Temin joined Renato Dulbecco’s laboratory at Caltech in 1951 where he started his work in understanding the genetics of Fruit Flies, but soon shifted his focus to RSV with the aim of creating cancer in a petri-dish, and did indeed manage to do so. He was able to infect a few cells in a petri-dish with RSV and incite them to multiply and grow uncontrollably. This was groudnbreaking, if you did not realize it yet, this means that everything that is needed for the malignant process of cancer to begin and do it’s dirty work, all of it is present inside the cell itself.</p> <h4 id="be-my-host">Be my host</h4> <p>Having separated the devil from the host, Temin could now study cancer in ways that had not been thought possible, and boy was he in for a surprise… As any basic biology book would tell you, when a virus enters it’s host, it infects the cell, produce more viruses and then infect more cells. Nowhere in the process is the cell’s genetic makeup altered. But RSV behaved differently, the virus had physically ‘attached’ itself to the cell’s DNA structure, causing it to multiply uncontrollably, which was unprecendted.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/cancer1/dna_alter-480.webp 480w,/assets/img/post_ims/cancer1/dna_alter-800.webp 800w,/assets/img/post_ims/cancer1/dna_alter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/cancer1/dna_alter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.zeclinics.com/blog/what-is-gene-editing-and-how-does-it-work/">src1</a></p> <p>The reason this was both surprising and confusing to Temin and his companions was that like many viruses RSV was also known to carry it’s genetic makeup in the form of RNA. Usually, it is this RNA that directly translates into viral proteins when a virus infects a cell and result in several copies of the virus from within the cell itself WITHOUT making any changes to the DNA makeup of the host cell. So how could a copy of the RSV’s genes convert to DNA, it was a well established fact in Cell Biology at that time that this was impossible.</p> <p>But just as every groundbreaking discovery, this hypothesis from Temin would need some solid backing and he had none at the beginning. His was the job now to isolate the enzyme and obtain the evidence for this reverse flow of information from RNA to DNA. As interesting as the detailed story is, we wouldn’t dive into it here, but he and his colleagues did manage to isolate the enzyme responsible for this particular effect which was Temin’s evidence, evidence that RSV was no ordinary virus, it was infact a <em>Retrovirus</em>.</p> <h3 id="the-wall-of-silence">The Wall of Silence</h3> <p>Now the discovery made my Temin and a contemporary researcher was ground-breaking, and two reports containing very similar findings were published in Nature Magazine in 1970, and it was immediately hailed by Cancer Researchers as the default mechanism behind cancer. But this seemed to have no effect whatsoever on the cancer oncologists, and very little changed in how Cancer was treated and dealt with, even in the minds of people like Sidney Farber(the father of modern chemotherapy) who had attended the conference, this discovery offered little to no help in how Cancer could be treated.</p> <h3 id="the-bubble">The bubble</h3> <p>By this time, there were two prevalent theories on the cause of cancer - Exogenous and Endogenous. The Exogenous theory was basically the viral theory of cancer i.e. Cancer was caused by viruses, while the Endogenous theory looked inwards - at genes and the DNA as a possible causal agent for cancer. Temin’s discovery hinted that an RNA virus could enter a cell, make a DNA copy of it’s genes, and then attach that to the cell’s genome. This led one virologist - Sol Spiegelman to conjure an interesting theory.</p> <p>Spiegelman suggested that over multiple generations these viral genes may get incorporated into a host’s genome and then get activated by a then unknown mechanism, and cause the cell to multiply uncontrollably. This theory was so tantalizing that Spiegelman set out to find retroviruses in almost all forms of cancer and more often than not, he ended up finding them. As more and more funding poured into this supposedly ground-breaking venture, Spiegelman managed to find more and more retroviruses which triggered even more funding, finally - people thought, a definite cause and hence a cure for the beast was within reach.</p> <p>When labs all aroudn the U.S. tried to replicate Spiegelman’s findings, it was realized that in the frenzy of finding retroviruses and having already established them as a definite cause for cancer in his mind, Speigelman had found retroviruses in places they did not exist. All the money could not make Speigelman’s findings fly, but the discovery of retroviruses and the frenzied studies on them were definitely not for nought, as Spiegelman would take his last breathes, they would in the wake of a strange illness among gay men and people with blood transfusions, and an year after Spiegelman passed away, it would be a retrovirus that would rise up as the causal agent for the disease - the Human Immunodeficiency Virus or <strong>HIV</strong>, but that’s an article for next Sunday.</p>]]></content><author><name></name></author><category term="Posts"/><category term="Cancer,"/><category term="Book"/><summary type="html"><![CDATA[A teensy tiny story from our everlasting battle with cancer.]]></summary></entry><entry><title type="html">The flow of Generative Networks</title><link href="https://w-ok-e.github.io/blog/2025/gnn/" rel="alternate" type="text/html" title="The flow of Generative Networks"/><published>2025-07-01T14:24:00+00:00</published><updated>2025-07-01T14:24:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/gnn</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/gnn/"><![CDATA[<p>Recently, I have been trying to generate drug samples using generative flow architectures and by now I have gotten accustomed to my prof. nodding in utter disappointment at the samples that my models generate. Which makes me question the fact that when models like Dall-E and stable diffusion can generate such a wide variety of images, what is the bottleneck in trying to generate chemical molecules from a given sample of similar drugs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_1.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_1.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="but-what-are-generative-flow-networks-aka-gflownets">But what are “Generative Flow Networks” a.k.a. GFlowNets</h2> <p>Generative Flow Networks are quite a recent phenomena, and have been inspired from Reinforcement Learning and Deep Learning and personally for me, it was quite a task to grasp the concept behind them. Primarily because GFlowNets are not just an independent concept, but rather a mixture of a host of different Machine Learning Concepts. One of them being Graph Neural Networks. So I feel it would suffice to kinda delve into Graph Neural Networks here and maybe cover GFlowNets in another one, we’ll see.</p> <h3 id="graph-neural-networks">Graph Neural Networks:</h3> <p>It would be a very good idea to start with what a graph actually is. Now to most of us, they might seem abstract, but graphs pop up almost everywhere you can find entities(nodes) that are related among themselves(depicted by edges) via some pre-defined notion.</p> <p>Now normal graphs and networks can only represent relations to a certain extent, and we can additionally specialize them via the concept of directed and undirected edges. We have obviously read and seen graph data in context of social networks or citation networks(Scientists citing each other) but there are a few interesting places where graphs tend to yield some insightful patterns and ideas when used.</p> <p>For instance Images, now obviously using graphs to represent images sounds totally absurd and useless. Because Images have a very nice structured pattern to them, that is one reason why they are arranged in 2d or 3d arrays as bands. But picture this, what if we were to represent every pixel in the image as nodes with the adjacent pixel(maybe even of the different color channel) forming the neighbors that are connected via appropriate edges. Although very redundant, this actually paints a very nice picture if you think about it in a particular way. There is a representation for graphs that is quite commonly known as the adjacency matrix, which is a way of representing nodes of a graph and their connections.</p> <p>So say there are 25 pixels in an image, we order them in a 5 x 5 matrix and fill the entries in the matrix such that they represent the edges shared between two nodes i.e. pixels in the case of an image.</p> <p>Now it doesn’t matter whether you love math or graphs or not, If you have a soul, you have to appreciate the underlying patterns that are popping up here. But there is only so much beauty one can appreciate, because when the question of efficiency comes, this is clearly not the best choice out there.</p> <h3 id="going-beyond-beauty">Going beyond Beauty:</h3> <p>In the wild though, graphs find application in not just the most beautiful of domains, but also the useful ones. Take heterogenous data like molecules, they are the building blocks of matter with electrons and atoms hanging in 3d space joined to their brethren via bonds and that too different kinds of - Single/Double/Covalent/Ionic. It’s a very convenient and common abstraction to describe molecules then, as Graphs! With nodes representing molecules and edges representing covalent bonds.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_2.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_2.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://distill.pub/2021/gnn-intro/">source</a></p> <p>Graphs can help us make sense of the most cluttered up data like in the case of social networks, which can be very insightful in figuring out patterns and collective behavior of people and of entities that are inter-connected.</p> <p><strong>Confused?</strong></p> <p>Alright so we have discussed a few use cases where we can use graphs to represent the data but what after that? What tasks can we perform on these collections once they are represented as graphs and how?</p> <p>So let’s first take a look at the tasks that we can perform on the graphs once we have represented the required data using them, and then dive into how GNNs can make the task simpler.</p> <h4 id="graph-level-task">Graph Level Task</h4> <p>First of many tasks that can be performed are “graph level tasks” i.e. looking at the graphs as a whole and then predicting the property of the entire graph. For instance, predicting how a molecule smells like based on the graphical representation of the molecules that we have is a graph level task. Or to draw a rough analogy, a graph level task would be akin to trying to classify certain images of the dataset like CIFAR 10, while with text, a similar problem is sentiment analysis where we want to identify the mood or the context of a given token of sentence.</p> <h4 id="node-level-task">Node Level Task</h4> <p>A node level task is associated with trying to predict the properties of a node in a graph, for instance you can imagine a small social circle as a graph and then a node level task there would be to classify each node to be belonging to a certain class.</p> <h4 id="edge-level-task">Edge Level Task</h4> <p>Another important aspect that we would like to deal with is classifying the relationship between various objects present in an image. That can fall under an edge level task. Consider a image that depicts a pitcher on a baseball field. If we consider all the entities present in the image as nodes, and then represent connections between them as edges of a graph, then one of the many relations that these edges can depict is the “action” between any two objects. For instance the pitcher and the hitter can be connected as “playing”.</p> <h3 id="a-suitable-representation">A suitable representation</h3> <p>But to perform all of these awesome tasks, we would need a representation for these graphs that would let us work with them in a mathematical setting where it’s more about numbers than pictures. ‘Cause even though the enamel of your teeth is harder than steel, we don’t use it in construction(Ha Ha bad joke)</p> <p>Alright so taking a step back, we are looking for a mathematical or a computer scienc-ish representation of graphs, so then we think about what information about a graph do we need to capture - Edges and Nodes, and which nodes are connected and by which edges.</p> <p>One possibility is the good old <strong>Adjacency Matrix</strong>, so consider a example below:</p> <div class="row mt-3" style="max-width: 500px; margin: 0 auto;"> <div class="col-md-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/Graph.avif" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/Graph.avif" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-md-6 mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_3.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_3.webp" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the matrix, the entry 1 indicates a connection between the corresponding nodes in the Graph. Now although this is a very nice representation, it’s not very memory efficient. Add to that the fact that if the position of any of the nodes is switched, the matrix completely changes, so it would be like playing dice while using these matrices as inputs to a model.</p> <h4 id="adjacency-lists">Adjacency Lists</h4> <p>It’s almost as if we have the sibling of an Adjacency Matrix to help us out now but this time the memory usage is very efficient. This time we use a tuple to capture what nodes are connected, so for instance from the previous graph, the nodes 2,4 are connected, so the adjacency list would contain the tuple (2,4) in it, hence the above graph can simply be represented as :</p> <p>[[1,4],[2,4],[3,4]], now notice how even if we change the order, it doesn’t make a difference, i.e. the list [[2,4],[1,4],[3,4]] represents the same information as the former. This nice property is called “Permutation Invariance”.</p> <p>Talking about a graph level task, we need efficient transfer of data between two nodes of the graph to be able to make complex predictions.</p> <p>This technique or message-passing is the primary technique methodology behind all of the things that we plan to accomplish using a Graph Neural Network.</p> <p>Information from each node/edge is collected and then aggregated using some function before being reapplied to the whole graph area and updating the information. But this turns out to be an issue for nodes/edges that are far apart because messages take longer to transmit even though we apply message passing multiple times across those nodes.</p> <h2 id="graph-neural-networks-1">Graph Neural Networks</h2> <p>So let’s talk about Graph Neural Networks then, with all the information about the graph loaded into the adjacency list(adjacency list is better as the order doesn’t change the information expressed). We will be discussing about the simplest GNN architecture which use the method of message passing to do the required tasks.</p> <p>But what in the wild world is message passing!????</p> <p>Okay I will break it down a bit, so imagine that you treat a molecule’s structure as a graph, then the information or essentially the numbers present in the nodes could tell you something about the atoms, those in the edges could tell you something about the bonds and when taken together as an aggregate, they convey something about the whole graph or the molecule. So the idea of message passing is for a node in the graph to kinda collect these numbers(of it’s own and those of it’s neighbors), aggregate them(via some function maybe mean or sum) and then update the other nodes about the same.</p> <p>But this turns out to be an issue for nodes/edges that are far apart because messages take longer to transmit even though we apply message passing multiple times across those nodes.</p> <p><strong>Solution to the problem of message transfer</strong></p> <p>One of the possible ways in which we could tackle the issue of message passing between far away nodes, is by using global representation of a graph or something called the context vector.</p> <p>The global context vector is connected to all other nodes and edges of the network and can act as a communicator between the nodes and the edges. So you can think of it as the internet that connects that two places far apart on the globe, and allows seamless exchange of information between two parts of the graph, which allows the representation to be sort of more complete and more connected in a sense.</p> <p>Then the simple magic of Graph Neural Network is that this representation is passed through a them to learn the required representation. So the things learned could very likely be “what numbers in the nodes/edges allow me to represent a paracetamol molecule” etc.</p> <p>An important fact here to keep in mind is that the network doesn’t make any changes to the number of nodes, edges in the graph. So the amount of information required to represent the output graph is the same as that required to represent the output graph, the only difference is that the embeddings have been updated now.</p> <p>Now this seems very simple but we can also have some information missing from either the nodes or the edges sometimes, and then in that case if we are supposed to apply a neural network on the node embeddings, we need to “pool” the data. So what basically happens is every node gathers information from it’s surrounding neighbors via a process called pooling and aggregates it(yes you got it right, message passing) and then the neural network or the function is applied on the aggregated data.</p> <p>Now whether we choose to transfer data from nodes to edges or vice versa is something that needs to be looked into because the two embeddings need not necessarily be of the same size, so it is not very obvious as to how to directly combine them. We can again use a neural network to map from one embedding to the other or maybe concatenate multiple embeddings together(Don’t worry, think of embedding as simply a vector or even simpler a collection of numbers that represent something about the associated entity). What’s also important is the way in which information is updated. Remember how we talked about updating the embedding of the edges and the nodes, what also matters is the order in which these are done. These design decisions among others(number of nodes, degree of each node etc.) are a bunch of factors that go into making efficient Graph Neural Networks.</p> <p>But wait Om, you haven’t yet touched upon the fact of how predictions are made and what on earth are these Graph Neural Networks doing with this message passing yada yada yada….</p> <p>Alright so let’s consider the following task: we have a bunch of drug molecules and their effectiveness listed against a disease(umm…say tuberculosis). What we can try to do is to have a model that could predict the effectiveness of newer molecules tuberculosis. So in order to leverage the power of GNNs, we would ideally need to express our molecules as graphs first and then pass them through the network.</p> <p>So consider the following molecule:</p> <div class="row mt-3" style="max-width: 1000px; margin: 0 auto;"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_4.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_4.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Source: Me 🤡
</code></pre></div></div> <p>The way we could go about representing this as a graph can vary, we can either consider individual atoms as nodes or some smaller fragments of the molecule as nodes, in which we would need to specify the index of the node that other fragments can connect to. So let’s say we take a CO fragment from the molecule the vector representing the positions where you can connect to can look like <a href="since you can attach two more atoms to the carbonyl carbon">0,0</a> and you can maybe have a feature vector that represents some additional information about the particular molecule. Need an example for that too??? Well C’mon &gt;_&lt;… okay okay I will give you one…</p> <div class="row mt-3" style="max-width: 400px; margin: 0 auto;"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_5.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_5.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Source: Me 🤡
</code></pre></div></div> <p>Consider you choose to represent features like toxicity, pH, polarity etc.. about every fragment/atom/node you have with you. So you will create a vector, that looks something like [0.5,0.4,0.9,…..] with the numbers in the vector representing those particular features about the fragment. This is the embedding….that I have been screaming about throughout this post. You can maybe try having a similar feature vector for the bonds as well, then comes in the pooling operation where you do crazy shit(nah just gather all the vectors together, put them into a matrix after applying either sum or mean function). Then pass this and the other nodes through the neural network to learn the relation between each fragment and how they affect the molecules’ effectiveness against the particular disease. So in the end output of our model is single number which could either be MIC(how well it inhibits a bacteria) or some other metric.</p> <p>After which you have the good old backpropagation to learn the feature vectors. Now of course there are other tricks up our sleeves which we can leverage to make these predictions better, like also utilizing the information about the entire graphs and the connections rather than just it’s nodes and edges, and also what types of graphs we choose to represent the data matters. But as we have seen, Graphs in general can be complicated sometimes, and life I feel is nothing but a graph, and it’s best traversed when we do it one node at a time(❁´◡`❁)</p> <p>PS: If you find any errors in the above writing, I beg you to shatter my delusions at okhere21@gmail.com</p>]]></content><author><name></name></author><category term="Posts"/><category term="GNNs"/><summary type="html"><![CDATA[A brief intro to Graph Neural Networks]]></summary></entry></feed>