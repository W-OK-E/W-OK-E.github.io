<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://w-ok-e.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://w-ok-e.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-25T17:33:54+00:00</updated><id>https://w-ok-e.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A simple and intuitive guide to using uv - an awesome tool from astral!</title><link href="https://w-ok-e.github.io/blog/2025/uv_tutorial/" rel="alternate" type="text/html" title="A simple and intuitive guide to using uv - an awesome tool from astral!"/><published>2025-10-26T18:00:00+00:00</published><updated>2025-10-26T18:00:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/uv_tutorial</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/uv_tutorial/"><![CDATA[<h2 id="step-1---setting-up-uv">Step 1 - Setting up uv</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-LsSf</span> https://astral.sh/uv/install.sh | sh
</code></pre></div></div> <h2 id="step-2---setting-up-a-simple-environment-in-uv-and-reproducing-it">Step 2 - Setting up a simple environment in uv and reproducing it</h2> <p>Clone the repo:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/W-OK-E/UWe.git
</code></pre></div></div> <p>This repo follows the following folder structure:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>UWe
|---UWe is the Parent project with some deps 
|
<span class="nt">----chimera</span>
|   |
|   <span class="nt">-----It</span> has non-conflicting deps with the parent folder
|---hunter
|   |
|   <span class="nt">-----It</span><span class="s1">'s dependencies are conflicting with the parent folder
|---sapphire
|   |
|   -----It'</span>s dependencies are conflicting with the parent folder but not folder chimera
</code></pre></div></div> <p>Each folder is designed to demonstrate the effectiveness of uv in isloating and manageing environments at lightning speed. But first we will start with a simple environment creation. Initialize a uv project in the root of the repo</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv init

</code></pre></div></div> <p>The command basically creates a .lock file and a .toml file which track the dependencies of your project.</p> <p><strong>Install the dependencies</strong> There are two ways in which this can be done</p> <ol> <li>Safe way <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv add <span class="nt">-r</span> base_requirements.txt
</code></pre></div> </div> <p>This command installs the dependencies in a .venv folder in the same directory, while also adding it to the .toml and .lock files so that we can keep track of the installed packages.</p> </li> <li>Installation with no tracking <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv pip <span class="nb">install</span> <span class="nt">-r</span> base_requirements.txt
</code></pre></div> </div> <p>The above command will only install the package in the .venv folder without adding it to the .toml and .lock files.</p> </li> </ol> <p>Now it might not make much sense but let’s test what each of those methods mean for your projects:</p> <h2 id="the-safe-way">The Safe Way</h2> <p>Once you have installed base_requirements.txt via the Safe Way, go ahead and remove the .venv in the root of your project:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">rm</span> <span class="nt">-rf</span> .venv
</code></pre></div></div> <p>This removes all the installed packages. Now run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run tests.py
</code></pre></div></div> <p>tests.py just imports a few packages to test the environment. What uv does now is, before running the script it checks the .lock and .toml files for what packages should be present in the environment, and automatically installs them without you having to explicitly mention it. Magic!! Which means that if you have installed the dependencies via the first method and someone else where to clone the repo, they won’t need to follow a tedious environment setup, they can just proceeed to run the relevant files and the packages are automatically installed.</p> <h2 id="the-direct-way">The Direct way</h2> <p>First remove all the config files by running:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">rm</span> <span class="nt">-rf</span> .venv uv.lock pyproject.toml 
</code></pre></div></div> <p>Initialize the project</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv init
</code></pre></div></div> <p>Now a crucial aspect of trying to run <code class="language-plaintext highlighter-rouge">uv pip install -r base_requirements.txt</code>, is that you need to initialize the project and create a .venv by running:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv venv
</code></pre></div></div> <ul> <li>If you donot run <code class="language-plaintext highlighter-rouge">uv init</code> uv will start using any environment it finds which will be disastrous</li> <li>If you donot run <code class="language-plaintext highlighter-rouge">uv venv</code> uv won’t know where to pip install the requirements.</li> </ul> <p>Once that is done, install deps via:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv pip <span class="nb">install</span> <span class="nt">-r</span> base_requirements.txt
</code></pre></div></div> <p>This time the packages won’t be tracked by .toml or .lock files so when you do:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">rm</span> <span class="nt">-rf</span> .venv
</code></pre></div></div> <p>And run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run tests.py
</code></pre></div></div> <p>You will get an error saying package not found, because uv doesn’t know what deps it needs to install.</p> <p>That completes everything about setting up a basic uv environment</p> <h2 id="managing-sub-projects">Managing Sub-projects</h2> <p>Now let’s see the following cases:</p> <ol> <li>If you have sub-projects inside your project that have non-conflicting dependencies with the parent project.</li> <li>If you have sub-projects with conflicting deps with the parent project</li> <li>If you have a sub-project which has conflicitng deps with th parent project but can work with the deps of some other sub-project.</li> </ol> <h3 id="case-1">Case-1</h3> <p>So this is the easiest scenario, the sub-folder chimera has deps which donot conflict with that of the parent folder, so we can just do:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>chimera
uv init <span class="c">#Optional</span>
uv add <span class="nt">-r</span> requirements.txt
</code></pre></div></div> <p>The deps for chimera donot conflict with that of the parent project so the packages will get installed to the <code class="language-plaintext highlighter-rouge">../.venv</code> folder and the packages will get added to the <code class="language-plaintext highlighter-rouge">../uv.lock</code> and <code class="language-plaintext highlighter-rouge">../pyproject.toml</code></p> <h3 id="case-2">Case-2</h3> <p>The deps in the hunter folder are incompatible with that of the parent project so if you:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>hunter
uv add <span class="nt">-r</span> requirements.txt
</code></pre></div></div> <p>You will get the following error:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  × No solution found when resolving dependencies:
  ╰─▶ Because <span class="nv">requests</span><span class="o">==</span>2.28.2 depends on urllib3&gt;<span class="o">=</span>1.21.1,&lt;1.27 and your project depends on <span class="nv">requests</span><span class="o">==</span>2.28.2, we can conclude that your
      project depends on urllib3&gt;<span class="o">=</span>1.21.1,&lt;1.27.
      And because your project depends on <span class="nv">urllib3</span><span class="o">==</span>2.2.0, we can conclude that your project<span class="s1">'s requirements are unsatisfiable.
  help: If you want to add the package regardless of the failed resolution, provide the `--frozen` flag to skip locking and syncing.
</span></code></pre></div></div> <p>Now the following things are <strong>not recommended</strong>:</p> <ol> <li>Doing a frozen install as the error suggests, it will be very hard to reproduce the environment later</li> <li>Running <code class="language-plaintext highlighter-rouge">uv pip install -r requirements.txt</code> - This will install the package to ../.venv without any syncing or locking, and will remove the parent folder’s package version. And everytime depending upon where you run a script from the package will swing between two versions because it will keep looking at the .toml that has the parent folder’s deps. And if you run the a script in hunter that requires <code class="language-plaintext highlighter-rouge">urllib3==2.2.0</code>, it will give you an error.</li> </ol> <p><strong>Recommended</strong>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>hunter
uv init <span class="nt">--no-worskpace</span>
uv add <span class="nt">-r</span> requirements.txt
</code></pre></div></div> <p>This will start treating the <code class="language-plaintext highlighter-rouge">hunter/</code> folder as a separate project.**If you run <code class="language-plaintext highlighter-rouge">uv init</code> without the <code class="language-plaintext highlighter-rouge">--no-workspace</code>, it will start treating <code class="language-plaintext highlighter-rouge">hunter</code> as a member of the parent workspace and give the same errors as above. Things to take care in this setup:</p> <ol> <li>If you want to run some script in the <code class="language-plaintext highlighter-rouge">hunter/</code> folder you must run it from that folder because uv looks up the immediate environment setup. So it uses the closest interpreter is used. Which means int he folder structure: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>UWe
|
.venv
tests.py
<span class="nt">----chimera</span>
|   |
|   <span class="nt">-----It</span> has some dependencies
|   hunter
|   |
|   .venv
|    tests.py
</code></pre></div> </div> <p>If you run tests.py in the hunter folder from the parent folder it uses the packages in the parent folder’s .venv. So to use hunter’s venv you must:</p> <ul> <li>either run it from the hunter folder</li> <li>or activate the hunter virtual environment via: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source </span>hunter/.venv/bin/activate
</code></pre></div> </div> <p>And when running scripts, you will need to pass the <code class="language-plaintext highlighter-rouge">--active</code> flag:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run <span class="nt">--active</span> hunter/tests.py
</code></pre></div> </div> </li> </ul> </li> </ol> <h3 id="case-3">Case-3</h3> <p>The deps in the sapphire folder conflict with the parent folder but are compatible with the deps of hunter/ folder, so you can use hunter’s environment globally but you will need to pass the <code class="language-plaintext highlighter-rouge">active</code> flag everytime:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source </span>hunter/.venv/bin/activate
</code></pre></div></div> <p>Then whatever you run a script pass the <code class="language-plaintext highlighter-rouge">--active</code> flag as:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source </span>hunter/.venv/bin/activate
uv run <span class="nt">--active</span> sapphire/tests.py 
</code></pre></div></div> <hr/> <p>This covers almost all of the intricate cases you might face while using uv, please note that while uv can completely replace pip, conda is still better in some cases, especially when your project has non-pythonic dependencies. For more details please head over to the documentation, it’s super cool - https://docs.astral.sh/uv/ <strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><em>**</em></p>]]></content><author><name></name></author><category term="Posts"/><category term="Computer"/><category term="Vision,"/><category term="OpenCV"/><summary type="html"><![CDATA[Quick Dive into Byte Pair Encoding tokenizers.]]></summary></entry><entry><title type="html">A quick overview of Byte Pair Encoding tokenizers!</title><link href="https://w-ok-e.github.io/blog/2025/bpe_short/" rel="alternate" type="text/html" title="A quick overview of Byte Pair Encoding tokenizers!"/><published>2025-10-20T18:00:00+00:00</published><updated>2025-10-20T18:00:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/bpe_short</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/bpe_short/"><![CDATA[<p>Tokenizers are algorithms that convert text into numbers for computers to understand, and today we will be convering one such tokenizer - BytePairEncoding tokenizers. These are the tokenizers used in LLMs like GPT - 2, llama-3 etc. Let us talk about the preliminaries first - “Bytes”!</p> <p>A bit is the smallest unit of information representing either 0 or 1. 8 bits make up one byte. Now what is the most naive way to sort of encode text? Just swap every character with a bit, or if we are talking “byte encoding” by a bit. Now even if you were to be a bit cautious and not have different encodings for the same words that come in a text, you would still end up having a mammoth numerical encoding of the given text input(Consider the fact that the english language alone has over a million unique words, so having a vocabulary of over a million words is well not the most optimal thing to do.)</p> <p>But okay you’d argue that with increasing compute power, this really shouldn’t be an issue, but if you look carefully under the hood the biggest downside of this approach is not with large chunks of text, but with small ones. Now if a paragraph with say 50 words needs 50 tokens to be encoded that is excessive. Ideally we would like to minimize this overhead for smaller chunks of text. So even though BPE Tokenizers do involve replacing text with numbers, they do so in a clever way:</p> <p>Consider the following chunk of text: <code class="language-plaintext highlighter-rouge">fishing at five to return by the fire.</code> Now If we were to tokenize the text with one token for every character, we would end up with 38 tokens, which is obscenely large for such a small piece of text. So instead of having a token for every possible character or every possible word, BPEs have a token for every possible subword, subwords that can in turn help you form complete words. Consider for instance the subword ‘th’, which alone can form the basis of many words such as ‘this’, ‘that’,’there’,’thought’,’theme’ etc. so your vast wordspace gets compressed into subwords that appear most frequently. Let’s take see how BPE tokenizer handles the above sentence <code class="language-plaintext highlighter-rouge">fishing at five to return by the fire</code>:</p> <ol> <li>Identify Frequent Pairs <ul> <li>In the given text,’fi’ appears thrice</li> </ul> </li> <li>Replace and Record <ul> <li>Replace ‘fi’ with a new token ID that is not already in use e.g. 256 Hence the new text is:<code class="language-plaintext highlighter-rouge">&lt;256&gt;shing at &lt;256&gt;ve to return by the &lt;256&gt;re</code> And simultaneously we also update the vocabulary: {0:..(some text),1…..256:’fi’}</li> </ul> </li> </ol> <p>We do several iterations of the above two steps and build up the vocablulary. So given a large enough corpus, we can have a vocabulary capable of tokenizing any arbitrary text. Detokenization or inverse tokenization is also pretty simple, we just apply the reverse of the encoding steps until we obtain the actual text.</p> <p>Okay but nothing is complete today unless we can see it working in code for ourselves. So we are gonna write a basic tokenizer which algorithmically lies along the lines of the BPE tokenizer used in GPT-2. So let’s build the intuition for that step by step:</p> <h3 id="identifying-frequent-pairs">Identifying frequent Pairs</h3> <p>The first step as discussed, was to identify the most frequent pairs which can be merged together, now if you have worked with text you will realize that doing this with text is cumbersome and hard and it would be a lot better if we had the strings in numeric form so given let’s take it as numbers shall we(For the Sake of simplicity outputs are preceeded by »&gt;): Given a text, say:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">A bunch of text</span><span class="sh">"</span>
</code></pre></div></div> <p>We will obtain the corresponding byte value for each character, coz of course if it is byte-pair encoding, but the reason for choosing bytes for this purpose is not arbitrary, bytes being the lowest rungs of the representation ladder which means they can encode anything and they can be reconstructed perfectly. Unlike unicode characters which can be inconsistent sometimes, having single or multiple codepoints for the same character(e.g. é). So bytes being a storage unit become language agnostic which means that the tokenizer can effortlessly handle any language or any symbols. Alright on with the implementation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">text_byte</span> <span class="o">=</span> <span class="nf">bytes</span><span class="p">(</span><span class="n">text</span><span class="p">,</span><span class="n">encoding</span><span class="o">=</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span>
<span class="c1">#or
</span><span class="n">text_byte</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">text_byte</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="sa">b</span><span class="sh">'</span><span class="s">A bunch of text</span><span class="sh">'</span>
</code></pre></div></div> <p>In terms of bytes:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">text_byte</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="mi">65</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">98</span><span class="p">,</span> <span class="mi">117</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">99</span><span class="p">,</span> <span class="mi">104</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">111</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">116</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">116</span><span class="p">]</span>
</code></pre></div></div> <p>So each number here is the byte representation of each character in the text i.e. 65 - A, 32 - ‘ ‘ etc. Now in order to carry out the process of replacement of pairs, we need to first figure out what pairs occur together most frequently.</p> <p>For a deep dive, please visit - <a href="https://sebastianraschka.com/blog/2025/bpe-from-scratch.html">BPEs</a></p>]]></content><author><name></name></author><category term="Posts"/><category term="Computer"/><category term="Vision,"/><category term="OpenCV"/><summary type="html"><![CDATA[Quick Dive into Byte Pair Encoding tokenizers.]]></summary></entry><entry><title type="html">Delegation, Discomfort and Decisions!</title><link href="https://w-ok-e.github.io/blog/2025/onurown/" rel="alternate" type="text/html" title="Delegation, Discomfort and Decisions!"/><published>2025-09-07T18:00:00+00:00</published><updated>2025-09-07T18:00:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/onurown</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/onurown/"><![CDATA[<p>This blog is gonna be rather short, suprisingly short actually - not just because I am busy juggling a bunch of things in my life at the moment, but also because this is more so a self-reflection than a full fledged analysis of a research paper or the latest update on my Quadruped task.</p> <p>The other day I was in the classroom, when someone brought up the idea of participating in a hackathon. And it was a reputed one so I gave a serious thought to participating in it. But then a thought quickly raced through my head, the thought that the people who were participating with me weren’t all that good, and me being me would end up doing most of the work even if it meant missing basketball - a sport that I love so dearly.</p> <p>But hey, it’s not just cases like these, I manage a team of over 40 people where I also head multiple projects. As the work begins I put people on todo boards and assign tasks, but then as time goes on, I begin taking up most of the work and with time I have realized it is because of the following reasons:</p> <ol> <li>The delay caused in assigning tasks, correcting their errors, explaining the workflow etc.</li> <li>The discomfort in asking them to do a certain task on top the existing tasks that they have been assigned.</li> </ol> <p>Okay valid, sometimes delegating tasks can slow things down, but then what I need to remind myself of the following things:</p> <ol> <li>The initial steps are slow, but with time, the people whom you assign tasks also grow, and the work becomes easier</li> <li>You have a better team to work with on new people, and also more time to do things that are meaningful.</li> </ol> <p>Then comes the decisions part, and more than just decisions, this segment is related to having ideas of your own. More often than not, I find myself being swept by the barage of suggestions and ideas, with little to no room for those of my own. These have the following repurcussions:</p> <ol> <li>When questioned I would have nothing better to say than “Oh It sounded right”, instead of being able to give a rational explanation about why the decision was made.</li> <li>Sometimes this leads to obscene amount of time being wasted, because you keep switching from idea to another, because all of them sound fascinating/brilliant, compounded by the lack of thoughts of your own.</li> </ol> <p>Now, don’t get me wrong, obviously suggestions from your team mates are invalulable and important, but what I want to stress here is the importance of having thoughts that are your own, and learning the art of delegating tasks in a way that help you and your team grow and learn.</p>]]></content><author><name></name></author><category term="Posts"/><category term="notes"/><category term="to"/><category term="self."/><summary type="html"><![CDATA[It's so easy to get lost in othe's thoughts or even our own ideas, is there a way to take this up? And why should we worry about whether]]></summary></entry><entry><title type="html">Watch and Learn, forget the speech!</title><link href="https://w-ok-e.github.io/blog/2025/saycan/" rel="alternate" type="text/html" title="Watch and Learn, forget the speech!"/><published>2025-08-31T18:00:00+00:00</published><updated>2025-08-31T18:00:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/saycan</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/saycan/"><![CDATA[<p>This is gonna be another one of the papers that I came across when working on integrating LLMs with the Quadruped, and it was rather an interesting find. While the ideas were intuitively simple, the methodology was definitely doing something unique and out of the box.</p> <h2 id="i-have-a-soul-but-no-body">I have a soul, but no body….</h2> <p>Providing instructions can be a bit tricky sometimes, mostly when we are not fully aware of the state/condition of a person whom we are trying to instruct. You could ask me how to boil an egg, and I would give you the steps to it, but that would also require me to know the resources you are working with, and your immediate surroundings or else the recipe will fall short. This applies more appropriately to Large Language Models breaking down complext actions for Robots, since the LLM lack any physical connection to the real world, their suggestions on how to do a particular task may come as very generalized or may turn out to be impossible to carry out(e.g. “Asking a robot to turn the AC on, but the room only has fans”). So this paper approaches this as sort of embodying the LLMs through a robot, to provide a better seed context for the LLM to produce informed actions.</p> <p>But the way they have proposed this embodiment is also rather interesting - each action that the robot can perform is given a certain ‘affordability’ value which is nothing but the probability that given the current state, how likely will that action result in a progress with the task. This score at each step makes the LLM aware of the robot’s ability to perform a certain sub-action. Hence the title of the paper ‘Say-Can’ in which the ‘Say’ part actually breaks a high level task into a sequence of sub-actions that can be performed to the task’s completion, while the ‘Can’ part is a learned affordance function that dictates the possiblity of carrying out the actions at every step.</p> <h2 id="getting-the-preliminaries-right">Getting the preliminaries right</h2> <p>Let’s talk about the components involved here. The first obvious thing are the LLMs, and to scratch the surface of what LLMs do - they basically generate the next token in a sequence based on a bunch of probability scores, that they obtain via the ever popular attention mechanism, this forms the ‘say’ part of the the whole pipeline. The next step is to be able to rank actions based on the robot’s given current state, and the go-to answer for the same was reinforcement learning, that too of a special kind - something called as Temporal Difference(TD) reinforcement learning. Here, the things are similar to your run-of-the-mill Markov Decision Process in terms of the reward function, the difference is in how the rewards are calculated, in non TD-RL the rewards are given at the end of an episode, while in TD, we sort of guess the future rewards that we might get, add them to the current reward and then evaluate our actions. This seems the right choice given the fact that the current action heavily depends on future actions, for instance you won’t decide on going to the museum tomorrow, if it’s a public holiday. And if you are someone who has read the classic paper of Reinforcement learning models learning the expression of Overall, the equation looks something like:</p> <p>\(V(s_t) \leftarrow V(s_t) + \alpha \big[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \big]\) Here, V() is the value function that represents how ‘good’ a particular state is, r is the reward of the current state, and $\gamma$ is the discount factor, which basically decides how much to weigh the future reward. And this happens at every step, unlike the Monte-Carlo method where:</p> <p>\(V(s_t) \leftarrow V(s_t) + \alpha \big[ G_t - V(s_t) \big]\) is calculated at the final step, where $ G_t $ is the cumulative sum of rewards after all the steps.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/say_can/td_mc-480.webp 480w,/assets/img/post_ims/say_can/td_mc-800.webp 800w,/assets/img/post_ims/say_can/td_mc-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/say_can/td_mc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://medium.com/data-science/a-comprehensive-overview-of-gaussian-splatting-e7d570081362">src1</a></p> <h3 id="the-problem-statement">The Problem Statement</h3> <p>Alright so we need the following things:</p> <ol> <li>Given a complete task description, we want the LLM to break it down into sub-actions.</li> <li>The sub-actions also have a few constraints : <ul> <li>The robot should be able to follow them</li> <li>The actions should lead to the successful completion of the task.</li> </ul> </li> </ol> <p>Now this is carried out in two steps:</p> <ol> <li>Every low level action that the robot can directly perform is given a textual label l. From the high level instruction, the LLM tries to generate a sequence of steps which are basically textual labels for the robot’s actions. The LLM also generates probability values for each textual label which dictates the validity of that action given the state <ul> <li>e.g. consider the task of getting a tool from the other end of the room, and the state of the bot being in the centre. Here, so the action of moving towards the tool would be given a higher probability by the LLM.</li> </ul> </li> <li>Regardless of how favorable a given action is given the current state, we also need to consider the fact that the choosing that particular action also leads to the task completion, which is dictated by a Bernoulli Random Variable.(A Bernoulli Variable is a yes or no random variable that tells us whether the task is completed or not)</li> </ol> <p>So given a task, the architecture SayCan combines the probabilities of skills being relevant to the task with the probability from a affordability function that dictates whether the skill will lead to the completion of the task, and a product</p> <h2 id="bringing-ideas-to-reality-robotics">Bringing ideas to <del>Reality</del> Robotics</h2> <p>Now, even though it’s the LLMs task to come up with do-able tasks for the robot. It must know what tasks can the agent actually do. So we provide the LLM with a bunch of actions, their language descriptions, a value function which calculates the ‘affordance’ of the action, because the model needs a place to start. Now comes the question of how we train the agent to perform these skills and obtaining the affordance of these skills.</p> <p>Two methods were empoyed for the same:</p> <ol> <li> <p>Imitation Learning - The agents are exposed to multiple instances of tasks being performed and learns effective low level policies from that, their performance is then evaluated by human actors which decides if the task was compelted successfully or not.</p> </li> <li> <p>Multi-Task Learning - A suite of tasks(Handling, manipulating objects, movement) are performed and the robot learns common functionalities across those tasks. So instead of training policies for single tasks, they train multi-task policies i.e. a policy that can solve more than one task(Jargon alert: A Policy is nothing but something that translates given real world inputs into an action)</p> </li> </ol> <p>So the crux of the matter is that the LLM breaks down the instructions into simpler sub-tasks. The LLM basically generates the language description of the sub-tasks, then they used a sentence encoder to generate an embedding for each sentence. These embeddings are then fed to the policy and the value function that jointly outputs what action would be best suited for that scenario.</p> <h2 id="training-the-low-level-skills">Training the Low-Level Skills</h2> <p>Just like any robotics problem we start with training the agents in the simulation, and the underlying Markov Decision Process consists of a reward function, as well as skill specifications that are used by the model. But in order to expand over this idea and train the agents at scale in real world and to learn a RL policy that adapts well to language they used <strong>MT-OPT</strong> in the <strong>Everybody Robots</strong> simulator using <strong>RetinaGAN</strong> sim-to-real transfer. The <strong>action space</strong> of the policies includes six-degrees of freedom of the end-effector pose as gripper open and close commands, x-y position and yaw orientation delta of the mobile base of the robot, and the <strong>terminate</strong> action.</p> <p>A wide variety of skills were chosen for the robotic arms to be trained on, most of which included the skills that would be posed in a kitchen environment.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/say_can/skills-480.webp 480w,/assets/img/post_ims/say_can/skills-800.webp 800w,/assets/img/post_ims/say_can/skills-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/say_can/skills.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://arxiv.org/abs/2204.01691">source</a></p> <p>Another advantage that this methodoloy carries is the freedom of choice when in comes to the LLMs that we use for the various tasks. The architecture is very modular and while we can use a different LLM for breaking down the higher level instructions into lower ones(planning), we can use a different one for obtaining the embeddings to each sentence. SayCan propses a unique approach to mapping higher level tasks into actions that are feasible, viable and appropriate for a robotic agent given a particular state.</p>]]></content><author><name></name></author><category term="Posts"/><category term="Deep"/><category term="Learning,"/><category term="Research,"/><category term="LLMs,"/><category term="Robotics,"/><category term="Quadruped."/><summary type="html"><![CDATA[The best way to learn something is to imitate the actor.]]></summary></entry><entry><title type="html">Lights❌ Language✅ Camera…Action</title><link href="https://w-ok-e.github.io/blog/2025/quarv/" rel="alternate" type="text/html" title="Lights❌ Language✅ Camera…Action"/><published>2025-08-24T18:00:00+00:00</published><updated>2025-08-24T18:00:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/quarv</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/quarv/"><![CDATA[<p>If you are here from the last one, you would be reeling from all the researchy stuff we navigated or rather skimmed through back there. But Hey, you never really get to understand ideas completely unless you do a deep dive. And that is what we are gonna do here, a full in-depth review of the Quar-VLA papers which we discussed in the last one. So welcome to another article where you lay back with your popcorn, and let me do the heavy lifting of breaking down the complex jargon for you.</p> <h2 id="well-its-not-that-dumb-to-be-honest">Well, it’s not that dumb to be honest</h2> <p>If you noticed one major criticism of the papers discussed in the last blog were that they were mapping high level commands to a bunch of small tasks and not really into low-level motor or sensor commands. This sort of leaves the headache of infering the low level actions from the high level commands to the action manager, which then also needs to be trained as to what exactly particular “words” or “commands” map to in the action space. This is partly addressed by the Quar-VLA paper, which deals directly in the Vision-Language-Action Space, and they have also developed a specific VLA model for the same along with a multi-task dataset. But first let us explore their methodology and approach in depth, shall we?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/Quar_VLA/vision_action-480.webp 480w,/assets/img/post_ims/Quar_VLA/vision_action-800.webp 800w,/assets/img/post_ims/Quar_VLA/vision_action-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/Quar_VLA/vision_action.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00808-supp.pdf">src1</a></p> <h3 id="vision-language-action-models">Vision Language Action Models</h3> <p>An important thing to undestand here that if we are planning to build a model capable of producing motor-level commands from Vision and Language, we also need to look after the frequency at which the commands need to be sent. Because in the case of a quadruped, the actions that we output can neither be too simplistic like the velocity commands given by the nav planners, or require a very high frequency similar to the motor level commands. Hence having a dataset that bridges this gap is also of prime importance and this is one of the things the authors have aimed at, and QUAR-VLA seems to be the first architecture that integrates Vision and Lanuage together to generate actions.</p> <p>To this end, we come to talk about <strong>Vision Language Action Models</strong> that integrate the visual input from the various sensors on the Quadruped along with the language commands. The end goal more or less is to train a conditional policy QUART that can interporet RGB Images, and high level commands. This policy takes RGB Images and the instructions as input and produces actions as output. Now in their paper specifically the authors aimed at a 11-dimensional space, and each action command looks something like:</p> \[\{ v_x, v_y, ω_z, θ_1, θ_2, θ_3, f, h_z, φ, s_y, h_z^{f} \}\] <p>Here, v<sub>x</sub>, v<sub>x</sub>, and ω<sub>x</sub> represent the velocities along the x-axis, y-axis, and z-axis respectively. θ<sub>x</sub>, θ<sub>x</sub>, and θ<sub>x</sub> indicate the gait pattern, f denotes the frequency, h<sub>x</sub> represents the height of the robot, φ denotes the pitch angle, s<sub>x</sub> corresponds to the foot width, h<sub>x</sub><sup>f</sup> represents the foot height, and t indicates the termination signal of the action.</p> <h4 id="generating-a-discrete-action-space">Generating a discrete action space</h4> <p>So an action policy is basically supposed to take in the images and the language input data and generate a bunch of action commands in the 11 dimensional action space. Now for each possible action(velocity, rotaion angle etc.) there is a continous range of values with a lower and upper bound which makes it very hard for the policy to reliably learn a set of actions given a particular input. Now this does not mean that we keep a set of discrete values, or else that will defeat the purporse of having a smooth action space as output. Discretization here means obtaining a set of bins for each possible action so that we can reliably say that okay “Velocity 5 m/s forward with rotation 30 degrees falls into this bin, choose a continous value from that. Effectively narrowing down the search space and “discretizing” our model’s outputs.</p> <ul> <li>Each action has a continous domain with a lower and a upper limit, this interval is divided into 256 bins of equal width = (range)/256</li> <li>And for any give target value, the corresponding index of the bin that it falls into is given by GIF(a - lower bound of the action space)/width of bins</li> </ul> <p>Alright, so given this action space, multiple sets of actions were tried each in a different domain - perception, navigation, manipulation etc. each with a varying level of difficulty on the task. Some involved simple object detection and identification tasks, while others involved more complex identiying, manupilating and moving objects from one place to the other, and an obvious trend was observed. For more complex tasks, we required larger number of episodes as compared to tasks that were relatively simpler.</p> <p>But what was more interesting was the insights on the relationship between the tasks that were assigned:</p> <ul> <li>Perception emerged as the a foudnational task, underpinning the other tasks</li> <li>Basic Navigation came next</li> <li>As an extension of the above two fundamental tasks, arose the four - Spatial Navigation, Environment Adaption, Obstacle Avoidance and Object Manipulation.</li> </ul> <p>The following picture from the paper does a nice job of illustrating the same:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/Quar_VLA/action_rel-480.webp 480w,/assets/img/post_ims/Quar_VLA/action_rel-800.webp 800w,/assets/img/post_ims/Quar_VLA/action_rel-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/Quar_VLA/action_rel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://arxiv.org/abs/2312.14457">src1</a></p> <h2 id="devil-in-the-details">Devil in the Details</h2> <p>I would like to list a few key aspects of the author’s approach, since I am working on mine as well, so if you have stuck around till this part of the blog, that’s terrific, let’s take note of a key constraint involved in the same:</p> <p><strong>Consistency Constraints</strong> - Sim to Real is one of the hardest problems out there in general. Forger about training a fancy quadruped. Transferring polcies from simulation to real life even for something as small as a robotic arm involves a lot of quirks, and the sheer amount of variables involved can be tumultous. Hence to maintain consitency between the simulation environment and the real world, the authors took a few key steps:</p> <ul> <li>The starting postion of the robot was always the origin, with the target randomly positioned between a set of fixed coordinates.</li> <li>The data collection setup adheres to a predefined language template where the task, the object, speed and the gait are pre-defined.</li> </ul> ]]></content><author><name></name></author><category term="Posts"/><category term="Deep"/><category term="Learning,"/><category term="Research,"/><category term="LLMs,"/><category term="Robotics,"/><category term="Quadruped,"/><category term="Vision"/><category term="Language"/><category term="Models."/><summary type="html"><![CDATA[Deep Dive into the Quar-VLA paper]]></summary></entry><entry><title type="html">Your dog can sniff, mine scans…</title><link href="https://w-ok-e.github.io/blog/2025/quad/" rel="alternate" type="text/html" title="Your dog can sniff, mine scans…"/><published>2025-08-17T18:00:00+00:00</published><updated>2025-08-17T18:00:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/quad</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/quad/"><![CDATA[<p>The title of this blog is kind of misleading, and I apologize for that. This is not about describing our Quadruped at Project MANAS a.k.a Sam, “scans” v/s “sniffs”, it is more about using that distinctive mechanical and electrical ability to improve it’s intelligence. And unless you have been living under a rock, you would know that the hottest intelligent systems in ‘da club right not are LLMs, so I thought to myself, what if we could integrate LLMs with the Quadruped and see how it improves it’s capacity to undertake various tasks.</p> <h2 id="well-its-not-that-dumb-to-be-honest">Well, it’s not that dumb to be honest</h2> <p>The Quadruped procurement for Project MANAS was actually the brainchild of our seniors and super-seniors who had interned at IISC Bangalore’s robotics laboratory. Now, being the powerhouse that IISC Bangalore is, among the various projects being worked upon, the Quadruped was one of them. So why not have our own test bench that allows us to learn about these systems, while at the same time developing and researching in an exciting domain.</p> <p>So it’s quite smart for a mechanical dog I would say, autonomous navigation is not it’s best suit I’d say, but it does manage to navigate to straight goals, it can follow certain hard-coded voice commands. But what really bugs me is that while you can ask Sam to Wiggle it’s butt and show you a dance, you can’t ask it to simply bring you a document that your friend wants you to have a look at. And I was like well, why not incorporate that system into it then? So I turned to the most intelligent(Supposedly) systems of today - LLMs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/dawg1-480.webp 480w,/assets/img/post_ims/dawg/dawg1-800.webp 800w,/assets/img/post_ims/dawg/dawg1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/dawg1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://medium.com/data-science/a-comprehensive-overview-of-gaussian-splatting-e7d570081362">src1</a></p> <h3 id="when-in-doubt-go-to-the-library-arxiv">When in doubt, go to <del>the library</del> arxiv</h3> <p>Okay so since we have just begun this venture, just like any good research project I put my head out to sniff if people have already done something similar, and viola did I find it. Every major corporation from Google to Nvidia(I was kinda sad I couldn’t read a boston dynamics paper but okay) is heavily invested in this and have been trying to incorporate LLMs into Robotic agents in order to make them smarter, in today’s blog, I will be briefly breaking down 5 such papers for you, listing out some key points along with what they achieved and what they missed out on. And ofcourse just like anything on the internet today, this list is incomplete without Google.</p> <h2 id="do-as-i-can-not-as-i-say">Do As I Can, not As I say…</h2> <p>Well at first, if you thought that I was trying to be funny with the above heading, I WASN’T, it’s an actual title to an actual paper by people at Google and Everyday Robotics. Computer Science researchers seem to be the most creative people when it comes to naming their works, well anyways what bells does the title ring when you think of it in terms of LLMs and Robotics? Okay I’ll let a picture from the paper explain it to you:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/can_say1-480.webp 480w,/assets/img/post_ims/dawg/can_say1-800.webp 800w,/assets/img/post_ims/dawg/can_say1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/can_say1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://arxiv.org/abs/2204.01691">source</a></p> <p>Now imagine if instead of giving instructions to the Robotic arm, the LLM was instructing you(Arghhh they’ll rule us) despite the order in which the LLM is giving the instructions or the content of these instructions, we have a natural intuition that tells us things such as the sponge has to be picked before going to the trash can, or that maybe the cup is still there, I need to remove it before cleaning the area. Therefore we are carrying the instructions forward as “we can” not as “it was told” to us. This highlights a crucial aspect that no matter how smart LLMs are, their responses, and they themselves are not grounded in the real world, solely because they have not interacted with the actual environment.</p> <p>This is what the SayCan architecture proposed in the paper addresses, as depicted in the picture above, and they do this by treating the feedback from the robot as tokens, essentially making the robot the limbs of the LLM that allows it to interact and gain feedback from the surrounding world.</p> <h4 id="work-encompasses">Work encompasses:</h4> <ol> <li>Leveraging the rich semantic knowledge in LLMs to complete real world tasks.</li> <li>It does so by treating all the possible things that the robot can do as “skills” for e.g. lifting an item, navigating to a location are all “skills” in this context.</li> <li>For any scenario that the architecture needs to plan for, each skill is assigned a certain “value” as to how appropriate that skill would be in that scenario. Simply imagine trying to develop a website, out of all the dev skills in your bag, certain skills like react, DBMS would be handy here and hence would be assigned higher values by the system.</li> <li>Each skill is given a text-label so that it can be easily parsed by the Language model and then logically arranged to form a cohesive response that is well grounded in environmental constraints and achievable.</li> </ol> <h4 id="research-gaps">Research Gaps:</h4> <ol> <li>Since the paper is quite old, and the LLMs were relatively small(in terms of parameter count) their abilities were limited by the training data.</li> <li>The range of skills(for instance a simple robotic arm which is fixed, can be dextrous but fail to follow navitation commands) that the agent has also poses a bottleneck.</li> <li>The paper does not explore robot planning and language, and other ways of combining language with robotic control.</li> </ol> <h2 id="can-you-just-break-it-down-for-me">Can you just break it down for me?</h2> <p>Well the above heading is my goto prompt these days to those Chatbots whenever I feel stuck in a mess of jargon and buzzwords. Oh wait! That makes me think, what if all our instructions are nothing but huge and confusing web of jargon and buzzwords to an agent. For instance when I ask Sam to “walk over to the guy wearing the red shirt with this doc”, could it be that it gets lost in the multitudes of possible actions that it can take but can’t decide for sure just because the action is not realy clear to it. That is where the paper : “Language Models as Zero Shot Planners” comes in, it leverages LLMs to break down high-level tasks into a bunch of smaller and actionable steps(i.e. actions closer to “skills” as described in SayCan paper).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/zero_shot1-480.webp 480w,/assets/img/post_ims/dawg/zero_shot1-800.webp 800w,/assets/img/post_ims/dawg/zero_shot1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/zero_shot1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.iconfinder.com/icons/10874016/robot_solution_robot_confusion_robot_problem_confused_person_worried_person_icon">source</a></p> <h4 id="work-encompasses-1">Work encompasses:</h4> <ol> <li>Investigates the possibility of breaking down high level tasks into smaller actionable steps.</li> <li>Instead of learning the mapping via step-by-step actions on “how to act”, it relies on the semantic information inside LLMs to extract that mapping.</li> <li>But don’t be misled,even though they donot rely on command-action pairs to learn the mappings, the LLMs are shown the mapping between textual representations of high-level and low-level commands. It’s like telling a child: “Go Play football” means “Put your shoes on, step out of the house, walk towards the field, play.” (Gosh kids these days….)</li> <li>This preconditioning improves the performance over LLM baselines.</li> </ol> <h4 id="research-gaps-1">Research Gaps:</h4> <ol> <li>Even though the LLMs can output logically correct commands, the absence of feedback from the environment reduces chances of smaller actions being actually executable. For instance, for the high level command: “Clean the room” -&gt; “grab the vacuum cleaner” might not be an executable step because the environment does not have one.</li> <li>Reliance on the belief that the simpler tasks that are generated by the LLM(i.e. mid-level actions) are already doable by the lower-level sensorimotor actions and that these lower-level action donot need any instructions from the LLM.</li> </ol> <h2 id="the-power-of-the-sun-in-the-palm-e-of-my-hand">The power of the sun, in the PaLM-E of my hand…</h2> <p>Well, if you recognized the above dialogue to be from Spider Man, kudos buddy both of us are spidey lovers! But instead if you recognized google’s LLM PaLM in there, hats off <em>~ ~</em>. See, one thing is sort of obvious now, that LLMs regardless of how good they are, struggle with acting under the constraints of the actual environment i.e. they are not grounded. Imagine them being like a super smart frog in the well who knows everything but struggles to make completely accurate decisions about being in a savannah simply because he has never been there. And if you want to understand the work done by the authors of PaLM-E you can imagine giving the frog extended sensors which sort of tell him the conditions in the Savannah. Their LLM takes sensor data, the state of the robot and textual input all as “sentences” ofc. with relevant encoding. And it made some decent leaps, as in being able to do sequential planning from textual input, however the wise reader would look behind the glam and see that being from google, the model is HUGEEEE, I mean 562B parameter. So it’s like sort of saying GPT 3.5 was better than GPT 3 because it had more parameters. Ofcourse that is not the case entirely here, as they way the model is trained also matters but it’d be interesting to see if lighter adaptations incorporating multiple sensors and sources exist. However some people may think about the parameter count of the chatbots we’ve come to know and love, and question whether 562B is really that huge…</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/llm_size.webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/llm_size.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.reddit.com/r/Infographics/comments/1c81n2o/the_size_of_llms_apr_2024/">source</a></p> <h4 id="work-encompasses-2">Work encompasses:</h4> <ol> <li>Injects continous sensory data from the robot model into the LLM, meaning the embedding space is multi-modal(basically saying data of different types is encoded to live in the same space where it can be used by the LLM)</li> <li>Capable of performing sequential manipulation task and does not need another LLM to break down the task into simpler ones. For instance asked to “Make a Cake Batter from all the ingredients you see”, it successfully decodes the sub-steps involved in the process like (Crack egg, put egg in bowl..etc.)</li> <li>Similar to the past papers, relies on the fact that the robot is well-versed in low-level policies i.e. the LLM won’t break down the action of “Crack egg” further into joint or motor commands, the “skills” of basic movement and handling are already learnt by the agent, and PaLM-E is only concerned with providing the high level commands in an autoregressive(one token at a time based on the past sequence) manner.</li> <li>Training on large scale vision language datasets, allows the model to be highly accurate in embodied tasks i.e. tasks which require awareness of your surroundings given that you are able to see if not feel it.</li> </ol> <h4 id="research-gaps-2">Research Gaps:</h4> <ol> <li>Parameter size is a big red flag that throws any question of practical deployement out the window.</li> <li>Again, similar to previous papers, reliance on the belief that the simpler tasks that are generated by the LLM(i.e. mid-level actions) are already doable by the lower-level sensorimotor actions and that these lower-level action donot need any instructions from the LLM.</li> <li>Performance with noisy sensor data across different environmental settings needs deeper evaluation.</li> </ol> <h2 id="what-if-you-gave-me-everything-in-one-place">What if you gave me everything in one place?</h2> <p>Well if you noticed something in the papers so far is that all of them generate a bunch of instructions for the agent to follow from the prompt that is given to them. So the structure looks something like a language command, which the LLM parses to generate a symbolc plan and then generate certain mid-level commands that the agent can follow based on the ‘skills’ or low-level sensorimotor movements. There seems to be a slight disconnect between the prompt to the LLM and the final actions performed by the agent, and you rely on the robot’s ability to correctly break down the tasks into the right set of low-level commands. This gap is bridged by Quar-VLA, which treates everything from the initial command to the final actions that the agent recieves as a single pipeline.</p> <p>So it is somewhat similar to the model actually providing explicit commands to he agent to carry out the actions that would help it achieve the high level semantic task. This can look like “Rotate Hip Joint by 30 degrees”, or “actuate the knee joint” etc.</p> <h4 id="work-encompasses-3">Work Encompasses</h4> <ol> <li>Proposed a new paradigm that integrates the vision input from the Quadruped’s sensor, the language ingretation and action generation.</li> <li>The interesting factor here was that this was not just limited to carrying out tasks, but also extended to navigation and manipulation which required much complex understanding of the environment. Like for instance if I want Sam to navigate in a room towards a red target, it must first be able to detect the target and move towards it.</li> <li>The also compiled all of the sensor data collected into a single database(QUARD) which can be utilised for end to end training for other Robotic agents.</li> </ol> <h4 id="research-gaps-3">Research Gaps</h4> <ol> <li>The environment can be improved - more terrrains, with different configurations.</li> <li>They don’t seem to have tested different path planning algorithms extensively, and for control they are only using a PD Controller, maybe we can check on the limitations for that, and improve it.</li> <li>Also, while they have shown extensive results in simulation, the results for actual environment might need further evaluations.</li> <li>Latency remains an issue, the response time can be improved.</li> </ol> <h2 id="our-tea-for-the-second-timepicked-by-a-bot">Our Tea for the second time…picked by a bot!</h2> <p>The RT-2 paper does something extra, in the sense the techniques adopted by the users developed good enough policies to allow the Quadrupeds to perform “unseen” tasks. The key to this was the training data employed by the authors. They not only trained on robot demonstration data(i.e. command-action pairs) but also extensive data from the web, which probably explains their ability to be able to carry out the unseen tasks. This also leverages the transformer architecture, which means it is a bit memory intensive, now if you are someone who is not familiar with the transformer architecture, hang tight, an explanation for that will be up soon here :) The usage of the transformer architecture means that the inputs and the outputs are handled as tokens(Just like how GPT or your fav LLM writes in “bits” sequentially, that bit-styled writing is a result tokenization)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/dawg/quad_in_sim-480.webp 480w,/assets/img/post_ims/dawg/quad_in_sim-800.webp 800w,/assets/img/post_ims/dawg/quad_in_sim-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/dawg/quad_in_sim.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="work-encompasses-4">Work Encompasses</h4> <ol> <li>Training on robotic demonstrations(sample robotic actions) as well as vast internet data</li> <li>From the prompt, the action commands are given out as discrete tokens(coz transformers) which means that continous actions(like rotate 30 deg while moving ahead) are given out as discrete tokens.</li> <li>Shows strong performance on semantic reasoning tasks.</li> </ol> <h4 id="research-gaps-4">Research Gaps</h4> <ol> <li>Low-level controls have not been addressed properly in this architecture as well, that is it is left to the agent to interpret mid-level commands accurately and execute the right low-level sensori-motor commands.</li> <li>Well, as awesome as the idea of training on the entire web sounds, it is a difficult mapping from the wide array of meanings one can extract from the various knowledge base of the internet versus the actual grounding that we would expect.</li> <li>Besudes, training models on such large databases requires extensive computing resources, and the need to complement simulation data.</li> </ol> <p>Pheww, that was quite a lot of unpacking, don’t you think? But what is awestriking is the speed with which progress is being made in this field, it’s almost always buzzing with some or the other activity. Now with a basic literature review underway, I’ll start setting up the environment for testing the integration of Quadrupeds out, so stick around for more updates, they are just around the corner ;)</p>]]></content><author><name></name></author><category term="Posts"/><category term="Deep"/><category term="Learning,"/><category term="Research,"/><category term="LLMs,"/><category term="Robotics,"/><category term="Quadruped."/><summary type="html"><![CDATA[Well, it was time I made our dog smarter, afterall the LLMs are tasked to do that aren't they?]]></summary></entry><entry><title type="html">Pictures of the Unseen…</title><link href="https://w-ok-e.github.io/blog/2025/difix/" rel="alternate" type="text/html" title="Pictures of the Unseen…"/><published>2025-08-10T18:00:00+00:00</published><updated>2025-08-10T18:00:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/difix</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/difix/"><![CDATA[<p>I have been recently pulled into the world of 3D reconstrucion while I was interning this summer at an awesome lab, and trust me the wild possibilities that exist in this space are just out of this world. I am huge fans of game engines and various high fidelity animations, but what was even more exhilirating was to be able to understand some of the basic principles and concepts behind these techniques, and then utilise them to bring 2d images to life. In this blog, we will be discussing about the various methodologies that I used for 3D reconstruction, and also an exciting aspect that we have taken up recently. So let’s get into it.</p> <h2 id="the-great-gauss-to-the-rescue">The Great Gauss to the rescue</h2> <p>What are the ways in which you can think of representing 2D images in 3D? PointClouds, or maybe structural sheets wrapped around a rough skeleton of the model. Now we will discuss pointclouds and meshes later in the blog, but first I wanted to talk about something very unique and awe-striking - <strong>Gaussian Splatting</strong>. Now I don’t about you, but having seen the number of places where the name of Carl Fredrick Gauss pops up, just blows my mind. No seriously, you should have a look at it, that dude is awesome.</p> <p>Okay let’s understand the basic idea first - basically what you have is multiple 3D blobs which are either initialized randomly or based on a sparse reconstruction of the final structure from the images we have(there is a step prior to this called SFM which I’m glossing over here, but that basically involves obtaining camera poses and matching image features from a set of given images.)</p> <p>These blobs or Gaussians(hail Gauss) too have some specific properties of “volumetric radiance fields”, which basically means that the gaussian blobs we have, are capable of modelling what colors will appear and what brightness when we look at it from certain angles for any point in 3D space(which if you know, sounds very similar to a NERF or Neural Radiance Fields concept-wise but they’re different in the way structures are represented), specifically they look something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/gs_nerf-480.webp 480w,/assets/img/post_ims/difix/gs_nerf-800.webp 800w,/assets/img/post_ims/difix/gs_nerf-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/gs_nerf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://medium.com/data-science/a-comprehensive-overview-of-gaussian-splatting-e7d570081362">src1</a></p> <p>But the real magic of Gaussian Splats is that they donot fall under the enraging umbrella of Neural Networks at runtime and hence beautifully illustrates how an algorithm that relies on classical methods and mathematics can still deliver great products. Although just to clarify, it does use backpropagation to optimize the parameters of the various gaussians that are initialized. So in brief once you have a sparse reconstruction of images, you take those 3d points as initial means for the Gaussians all of which have their own specific properties. These properties are then optimized via backpropagation.</p> <p>During rendering, we need to iterate through each and every pixel in the H x W image because each of them acted as a mean for the gaussian that was initialized and optimized. But again, it is important to note that during this rendering, this doesn’t pass through an MLP or a Neural Network unlike NERFs, which makes Gaussian Splatting a little faster during rendering.</p> <h2 id="why-make-a-mess-mesh">Why make a <del>Mess</del> Mesh?</h2> <p>Okay so it might have felt like blind worship of Gaussian splat in the last section(and it was because I am a huge fan of it) but Gaussian splatting also produces a lot of artifacts in the recosntruction. You can think of as when some blobs or gaussians are too elongted, and can appear spread over the place. Meshes on the other hand, can be imagined like a cloth wrapping around a skeleton structure and taking up the structural features of the model in question. This “cloth” is actually an oversimplification of many different elements that could be connected to each other in either regular or irregular patterns. Meshes although great at representing smooth features and edges, struggle with representing complex lightning conditions, while at the same time, even though gaussian splats can represent intricate details and various lightning conditions, they struggle with stray gaussians, especially at edges and in smooth areas. The following image really delivers the point of a mesh being a cloth stitched from various smaller subcomponents.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/cat_mesh-480.webp 480w,/assets/img/post_ims/difix/cat_mesh-800.webp 800w,/assets/img/post_ims/difix/cat_mesh-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/cat_mesh.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.artstation.com/marketplace/p/Ky67/cartoon-cat-base-mesh-3d-model">src</a></p> <h3 id="when-they-borrow-the-invisibility-cloak">When they borrow the invisibility cloak…</h3> <p>Now obviously there are many more methods for 3D reconstruction and still more variants of the same, and I could keep talking about them forever but this blog is about the missing pieces. Regardless of the methodology at play, one should not forget that 3D not just derives but relies on 2D data. If we donot have the snapshots of a particular scene/object from certain angles, it becomes very hard to model them. In case of Gaussian Splats,if we donot have ground truth captures for a model from certain angles, the mean points for gaussians in that particular region will be missing, which means that the gaussians surrounding that area will now try to fill that region up, which either means artifacts in the final reconstruction or poor splat generation.</p> <p>Here is a snapshot of a 3D reconstruction of a temple in Orissa - Somnath, which was done using fewer images than required:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/splat_decent-480.webp 480w,/assets/img/post_ims/difix/splat_decent-800.webp 800w,/assets/img/post_ims/difix/splat_decent-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/splat_decent.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>[source - Image from Author]</p> <h3 id="diffusing-it-away">Diffusing it away</h3> <p>Okay so let us first round out the problem that we have here - We are missing snapshots from certain angles or poses, and hence the view generated from is full of artifacts. What if, there was a way to take a snapshot of that view with artifacts and also obtain the camera pose along with it, because remember - for most 3D reconstrucion pipelines, we don’t just need the images, we also need the camera poses, so we also need a way to sort of first obtain the novel camera position. The approach we are gonna follow will be something like:</p> <ul> <li>Given the training or the ground truth camera positions already present in out dataset, we can interpolate to find camera positions that are not already there.</li> <li>From the novel camera poses that we obtain, we would then need a way to sort take a snapshot to get the novel view which might contain artifacts from the surrounding gaussians</li> <li>Once we have this novel view, we need to think of a way to get rid of these artifacts, and obtaining something closer to the ground truth view.</li> </ul> <p>Now I don’t about you, but in hindsight, to me this feels like an elegant idea that sounds so trivial. This is partly inspired from a few research papers I came across and also inputs from my professor and mentors.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/camera_poses-480.webp 480w,/assets/img/post_ims/difix/camera_poses-800.webp 800w,/assets/img/post_ims/difix/camera_poses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/camera_poses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Visualizing Camera Poses for a 3D model" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>[source - Image from Author]</p> <p>So having established the idea, let us see the ways in which we can attain the aforesaid goals. Now interpolating and taking snapshots can only happen once we have rendered the model as a gausian splat. And for the rendering part, we have NerfStudio, and using the python API we can even automate the process headless.</p> <p>We can interpolate from the existing camera positions and then rasterize the splats to obtain the snapshot, so that we end up with additional camera positions and their corresponding novel views, which can be used downstream for training. But there’s a catch, these images that we have obtained have artifacts so we need to find a way of fixing that.</p> <h3 id="controlled-diffusion">Controlled Diffusion</h3> <p>I am pretty sure you must have experimented with Ghibli art style when the trend was hot, but no we aren’t gonna talk about it today. We are gonna take up it’s predecessor - Stable Diffusion. Diffusion is capable of generating realistic images just from noise, essentially it learns that mapping of being able to “diffuse” noise in just the right amounts to end up with an image. But then if you have ever used any of those awesome demos online, you would have noticed that “A happy Koala made out of Blueberry Cake” doesn’t always yield the same image, there are variations to every generation, unless ofcourse we make the models deterministic by fixing the random samplers and the seed.</p> <p>And by now you must have guessed it, that if we plan to use a diffusion model to “fix” the artifacts or say “diffuse” them away predictively from our novel views, we will need some way of controlling the process. That is where another one of Deep Learning models come in - <strong>ControlNets</strong>!</p> <p><strong>ControlNets</strong> essentially aim at more informed or controlled image generation from diffusion models. Imagine your cat posed really beautifully the other day, and now you’re craving some more of the same 🐈. And ofcourse the mean creatures that they are, your cat is refusing to recreate the master-piece, add to that the utter difficulty it has been to explain the pose to a generative model. That’s where ControlNets come in handy, you can pass the image of your cat along with some other variants of it like Canny Edge, Depth etc. and then “condition” your model to then generate a cat in that particular pose.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/Control.webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/Control.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Visualizing Camera Poses for a 3D model" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://generativeai.pub/how-to-setup-controlnet-for-stable-diffusion-ai-step-by-step-guide-aafff8996719">src</a></p> <p>This miraculously allows us to condition diffusion models to “fix” the image containing artifacts by passing the nearest ground truth image, their depth maps(denoting the distance from the camera), the confidence map(what is the confidence score of each gaussian in the captured snapshot) along with the view that we need to fix, and viola, the model should fix this, and even though right now we are still working on this, I am super excited about this line of research, because just being able to spin models without even rendering them and them fixing those “broken” views via “controlled” generation just blows my mind, what about you?</p>]]></content><author><name></name></author><category term="Posts"/><category term="Diffusion,"/><category term="Deep"/><category term="Learning,"/><category term="Computer"/><category term="Vision,"/><category term="Research"/><summary type="html"><![CDATA[A short description of how we aim to improve 3D reconstruction using Diffusion Priors.]]></summary></entry><entry><title type="html">Is Cancer Modern or Retro &amp;gt;_&amp;lt; ?</title><link href="https://w-ok-e.github.io/blog/2025/cancer/" rel="alternate" type="text/html" title="Is Cancer Modern or Retro &amp;gt;_&amp;lt; ?"/><published>2025-08-03T14:24:00+00:00</published><updated>2025-08-03T14:24:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/cancer</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/cancer/"><![CDATA[<p>We have all known for quite some time now that Cancer is caused by mutations in the Human DNA which results in uncontrolled proliferation of the cell, resulting in tumors which sort of disrupt every function that the body is capable of. But believe you me, it was not always this simple. We still do not understand the exact mechanism behind this menace which is one of the reasons why it is so hard to come up with a cure. But today’s tale is not about the technicalities, rather it is about how once the research community was divided between viral and the genetic possibilities as potential causes for Cancer.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/cancer1/virus_vs_dna-480.webp 480w,/assets/img/post_ims/cancer1/virus_vs_dna-800.webp 800w,/assets/img/post_ims/cancer1/virus_vs_dna-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/cancer1/virus_vs_dna.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.dreamstime.com/illustration/dna-cartoon.html">src1</a> <a href="https://www.istockphoto.com/photos/coronavirus-animation">src2</a></p> <h2 id="but-why-look-at-viruses-at-all">But why look at Viruses at all?</h2> <p>If Alexander Fleming and other scientists had never managed to reproduce penicillin in subsequent experiments and tests once it appeared accidently on the petri dishes, society might still be plagued by even the simplest of ailments. Simply put, in order to better understand something or make use of the discovery, it must be easily reproducible in the laboratory, and in the late 1960s this was not the case with Cancer. Although the obvious culprits had been rounded out by then - Tobacco, asbestos, X-Rays, but I think that even you would agree that it is not the most scientific or the best method to induce cancer in a cell/subject by exposing it to lethal doses of X-Rays, or making a person smoke 20 cigarattes a day. They needed a reiable way of inducing caner in cells which can then be studied further.</p> <p>But there was one such candidate - <strong>Rous’s Sarcoma Virus</strong> was a rare virus that caused a rare cancer in a species of Chicken. And it’s namesake Peyton Rous, was already pushing Viruses as the key missing piece in the puzzle of Cancer’s causes. But little did he realise that this push would become the missing piece to a different puzzle than Cancer Viriology. But RSV came with it’s own caveats, the virus had never been isolated on a petri-dish to reliably study it. Until One day…</p> <h3 id="cancer-in-a-petri-dish">Cancer in a Petri-Dish:</h3> <p>Howard Temin joined Renato Dulbecco’s laboratory at Caltech in 1951 where he started his work in understanding the genetics of Fruit Flies, but soon shifted his focus to RSV with the aim of creating cancer in a petri-dish, and did indeed manage to do so. He was able to infect a few cells in a petri-dish with RSV and incite them to multiply and grow uncontrollably. This was groudnbreaking, if you did not realize it yet, this means that everything that is needed for the malignant process of cancer to begin and do it’s dirty work, all of it is present inside the cell itself.</p> <h4 id="be-my-host">Be my host</h4> <p>Having separated the devil from the host, Temin could now study cancer in ways that had not been thought possible, and boy was he in for a surprise… As any basic biology book would tell you, when a virus enters it’s host, it infects the cell, produce more viruses and then infect more cells. Nowhere in the process is the cell’s genetic makeup altered. But RSV behaved differently, the virus had physically ‘attached’ itself to the cell’s DNA structure, causing it to multiply uncontrollably, which was unprecendted.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/cancer1/dna_alter-480.webp 480w,/assets/img/post_ims/cancer1/dna_alter-800.webp 800w,/assets/img/post_ims/cancer1/dna_alter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/cancer1/dna_alter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.zeclinics.com/blog/what-is-gene-editing-and-how-does-it-work/">src1</a></p> <p>The reason this was both surprising and confusing to Temin and his companions was that like many viruses RSV was also known to carry it’s genetic makeup in the form of RNA. Usually, it is this RNA that directly translates into viral proteins when a virus infects a cell and result in several copies of the virus from within the cell itself WITHOUT making any changes to the DNA makeup of the host cell. So how could a copy of the RSV’s genes convert to DNA, it was a well established fact in Cell Biology at that time that this was impossible.</p> <p>But just as every groundbreaking discovery, this hypothesis from Temin would need some solid backing and he had none at the beginning. His was the job now to isolate the enzyme and obtain the evidence for this reverse flow of information from RNA to DNA. As interesting as the detailed story is, we wouldn’t dive into it here, but he and his colleagues did manage to isolate the enzyme responsible for this particular effect which was Temin’s evidence, evidence that RSV was no ordinary virus, it was infact a <em>Retrovirus</em>.</p> <h3 id="the-wall-of-silence">The Wall of Silence</h3> <p>Now the discovery made my Temin and a contemporary researcher was ground-breaking, and two reports containing very similar findings were published in Nature Magazine in 1970, and it was immediately hailed by Cancer Researchers as the default mechanism behind cancer. But this seemed to have no effect whatsoever on the cancer oncologists, and very little changed in how Cancer was treated and dealt with, even in the minds of people like Sidney Farber(the father of modern chemotherapy) who had attended the conference, this discovery offered little to no help in how Cancer could be treated.</p> <h3 id="the-bubble">The bubble</h3> <p>By this time, there were two prevalent theories on the cause of cancer - Exogenous and Endogenous. The Exogenous theory was basically the viral theory of cancer i.e. Cancer was caused by viruses, while the Endogenous theory looked inwards - at genes and the DNA as a possible causal agent for cancer. Temin’s discovery hinted that an RNA virus could enter a cell, make a DNA copy of it’s genes, and then attach that to the cell’s genome. This led one virologist - Sol Spiegelman to conjure an interesting theory.</p> <p>Spiegelman suggested that over multiple generations these viral genes may get incorporated into a host’s genome and then get activated by a then unknown mechanism, and cause the cell to multiply uncontrollably. This theory was so tantalizing that Spiegelman set out to find retroviruses in almost all forms of cancer and more often than not, he ended up finding them. As more and more funding poured into this supposedly ground-breaking venture, Spiegelman managed to find more and more retroviruses which triggered even more funding, finally - people thought, a definite cause and hence a cure for the beast was within reach.</p> <p>When labs all aroudn the U.S. tried to replicate Spiegelman’s findings, it was realized that in the frenzy of finding retroviruses and having already established them as a definite cause for cancer in his mind, Speigelman had found retroviruses in places they did not exist. All the money could not make Speigelman’s findings fly, but the discovery of retroviruses and the frenzied studies on them were definitely not for nought, as Spiegelman would take his last breathes, they would in the wake of a strange illness among gay men and people with blood transfusions, and an year after Spiegelman passed away, it would be a retrovirus that would rise up as the causal agent for the disease - the Human Immunodeficiency Virus or <strong>HIV</strong>, but that’s an article for next Sunday.</p>]]></content><author><name></name></author><category term="Posts"/><category term="Cancer,"/><category term="Book"/><summary type="html"><![CDATA[A teensy tiny story from our everlasting battle with cancer.]]></summary></entry><entry><title type="html">The flow of Generative Networks</title><link href="https://w-ok-e.github.io/blog/2025/gnn/" rel="alternate" type="text/html" title="The flow of Generative Networks"/><published>2025-07-01T14:24:00+00:00</published><updated>2025-07-01T14:24:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/gnn</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/gnn/"><![CDATA[<p>Recently, I have been trying to generate drug samples using generative flow architectures and by now I have gotten accustomed to my prof. nodding in utter disappointment at the samples that my models generate. Which makes me question the fact that when models like Dall-E and stable diffusion can generate such a wide variety of images, what is the bottleneck in trying to generate chemical molecules from a given sample of similar drugs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_1.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_1.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="but-what-are-generative-flow-networks-aka-gflownets">But what are “Generative Flow Networks” a.k.a. GFlowNets</h2> <p>Generative Flow Networks are quite a recent phenomena, and have been inspired from Reinforcement Learning and Deep Learning and personally for me, it was quite a task to grasp the concept behind them. Primarily because GFlowNets are not just an independent concept, but rather a mixture of a host of different Machine Learning Concepts. One of them being Graph Neural Networks. So I feel it would suffice to kinda delve into Graph Neural Networks here and maybe cover GFlowNets in another one, we’ll see.</p> <h3 id="graph-neural-networks">Graph Neural Networks:</h3> <p>It would be a very good idea to start with what a graph actually is. Now to most of us, they might seem abstract, but graphs pop up almost everywhere you can find entities(nodes) that are related among themselves(depicted by edges) via some pre-defined notion.</p> <p>Now normal graphs and networks can only represent relations to a certain extent, and we can additionally specialize them via the concept of directed and undirected edges. We have obviously read and seen graph data in context of social networks or citation networks(Scientists citing each other) but there are a few interesting places where graphs tend to yield some insightful patterns and ideas when used.</p> <p>For instance Images, now obviously using graphs to represent images sounds totally absurd and useless. Because Images have a very nice structured pattern to them, that is one reason why they are arranged in 2d or 3d arrays as bands. But picture this, what if we were to represent every pixel in the image as nodes with the adjacent pixel(maybe even of the different color channel) forming the neighbors that are connected via appropriate edges. Although very redundant, this actually paints a very nice picture if you think about it in a particular way. There is a representation for graphs that is quite commonly known as the adjacency matrix, which is a way of representing nodes of a graph and their connections.</p> <p>So say there are 25 pixels in an image, we order them in a 5 x 5 matrix and fill the entries in the matrix such that they represent the edges shared between two nodes i.e. pixels in the case of an image.</p> <p>Now it doesn’t matter whether you love math or graphs or not, If you have a soul, you have to appreciate the underlying patterns that are popping up here. But there is only so much beauty one can appreciate, because when the question of efficiency comes, this is clearly not the best choice out there.</p> <h3 id="going-beyond-beauty">Going beyond Beauty:</h3> <p>In the wild though, graphs find application in not just the most beautiful of domains, but also the useful ones. Take heterogenous data like molecules, they are the building blocks of matter with electrons and atoms hanging in 3d space joined to their brethren via bonds and that too different kinds of - Single/Double/Covalent/Ionic. It’s a very convenient and common abstraction to describe molecules then, as Graphs! With nodes representing molecules and edges representing covalent bonds.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_2.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_2.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://distill.pub/2021/gnn-intro/">source</a></p> <p>Graphs can help us make sense of the most cluttered up data like in the case of social networks, which can be very insightful in figuring out patterns and collective behavior of people and of entities that are inter-connected.</p> <p><strong>Confused?</strong></p> <p>Alright so we have discussed a few use cases where we can use graphs to represent the data but what after that? What tasks can we perform on these collections once they are represented as graphs and how?</p> <p>So let’s first take a look at the tasks that we can perform on the graphs once we have represented the required data using them, and then dive into how GNNs can make the task simpler.</p> <h4 id="graph-level-task">Graph Level Task</h4> <p>First of many tasks that can be performed are “graph level tasks” i.e. looking at the graphs as a whole and then predicting the property of the entire graph. For instance, predicting how a molecule smells like based on the graphical representation of the molecules that we have is a graph level task. Or to draw a rough analogy, a graph level task would be akin to trying to classify certain images of the dataset like CIFAR 10, while with text, a similar problem is sentiment analysis where we want to identify the mood or the context of a given token of sentence.</p> <h4 id="node-level-task">Node Level Task</h4> <p>A node level task is associated with trying to predict the properties of a node in a graph, for instance you can imagine a small social circle as a graph and then a node level task there would be to classify each node to be belonging to a certain class.</p> <h4 id="edge-level-task">Edge Level Task</h4> <p>Another important aspect that we would like to deal with is classifying the relationship between various objects present in an image. That can fall under an edge level task. Consider a image that depicts a pitcher on a baseball field. If we consider all the entities present in the image as nodes, and then represent connections between them as edges of a graph, then one of the many relations that these edges can depict is the “action” between any two objects. For instance the pitcher and the hitter can be connected as “playing”.</p> <h3 id="a-suitable-representation">A suitable representation</h3> <p>But to perform all of these awesome tasks, we would need a representation for these graphs that would let us work with them in a mathematical setting where it’s more about numbers than pictures. ‘Cause even though the enamel of your teeth is harder than steel, we don’t use it in construction(Ha Ha bad joke)</p> <p>Alright so taking a step back, we are looking for a mathematical or a computer scienc-ish representation of graphs, so then we think about what information about a graph do we need to capture - Edges and Nodes, and which nodes are connected and by which edges.</p> <p>One possibility is the good old <strong>Adjacency Matrix</strong>, so consider a example below:</p> <div class="row mt-3" style="max-width: 500px; margin: 0 auto;"> <div class="col-md-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/Graph.avif" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/Graph.avif" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-md-6 mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_3.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_3.webp" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the matrix, the entry 1 indicates a connection between the corresponding nodes in the Graph. Now although this is a very nice representation, it’s not very memory efficient. Add to that the fact that if the position of any of the nodes is switched, the matrix completely changes, so it would be like playing dice while using these matrices as inputs to a model.</p> <h4 id="adjacency-lists">Adjacency Lists</h4> <p>It’s almost as if we have the sibling of an Adjacency Matrix to help us out now but this time the memory usage is very efficient. This time we use a tuple to capture what nodes are connected, so for instance from the previous graph, the nodes 2,4 are connected, so the adjacency list would contain the tuple (2,4) in it, hence the above graph can simply be represented as :</p> <p>[[1,4],[2,4],[3,4]], now notice how even if we change the order, it doesn’t make a difference, i.e. the list [[2,4],[1,4],[3,4]] represents the same information as the former. This nice property is called “Permutation Invariance”.</p> <p>Talking about a graph level task, we need efficient transfer of data between two nodes of the graph to be able to make complex predictions.</p> <p>This technique or message-passing is the primary technique methodology behind all of the things that we plan to accomplish using a Graph Neural Network.</p> <p>Information from each node/edge is collected and then aggregated using some function before being reapplied to the whole graph area and updating the information. But this turns out to be an issue for nodes/edges that are far apart because messages take longer to transmit even though we apply message passing multiple times across those nodes.</p> <h2 id="graph-neural-networks-1">Graph Neural Networks</h2> <p>So let’s talk about Graph Neural Networks then, with all the information about the graph loaded into the adjacency list(adjacency list is better as the order doesn’t change the information expressed). We will be discussing about the simplest GNN architecture which use the method of message passing to do the required tasks.</p> <p>But what in the wild world is message passing!????</p> <p>Okay I will break it down a bit, so imagine that you treat a molecule’s structure as a graph, then the information or essentially the numbers present in the nodes could tell you something about the atoms, those in the edges could tell you something about the bonds and when taken together as an aggregate, they convey something about the whole graph or the molecule. So the idea of message passing is for a node in the graph to kinda collect these numbers(of it’s own and those of it’s neighbors), aggregate them(via some function maybe mean or sum) and then update the other nodes about the same.</p> <p>But this turns out to be an issue for nodes/edges that are far apart because messages take longer to transmit even though we apply message passing multiple times across those nodes.</p> <p><strong>Solution to the problem of message transfer</strong></p> <p>One of the possible ways in which we could tackle the issue of message passing between far away nodes, is by using global representation of a graph or something called the context vector.</p> <p>The global context vector is connected to all other nodes and edges of the network and can act as a communicator between the nodes and the edges. So you can think of it as the internet that connects that two places far apart on the globe, and allows seamless exchange of information between two parts of the graph, which allows the representation to be sort of more complete and more connected in a sense.</p> <p>Then the simple magic of Graph Neural Network is that this representation is passed through a them to learn the required representation. So the things learned could very likely be “what numbers in the nodes/edges allow me to represent a paracetamol molecule” etc.</p> <p>An important fact here to keep in mind is that the network doesn’t make any changes to the number of nodes, edges in the graph. So the amount of information required to represent the output graph is the same as that required to represent the output graph, the only difference is that the embeddings have been updated now.</p> <p>Now this seems very simple but we can also have some information missing from either the nodes or the edges sometimes, and then in that case if we are supposed to apply a neural network on the node embeddings, we need to “pool” the data. So what basically happens is every node gathers information from it’s surrounding neighbors via a process called pooling and aggregates it(yes you got it right, message passing) and then the neural network or the function is applied on the aggregated data.</p> <p>Now whether we choose to transfer data from nodes to edges or vice versa is something that needs to be looked into because the two embeddings need not necessarily be of the same size, so it is not very obvious as to how to directly combine them. We can again use a neural network to map from one embedding to the other or maybe concatenate multiple embeddings together(Don’t worry, think of embedding as simply a vector or even simpler a collection of numbers that represent something about the associated entity). What’s also important is the way in which information is updated. Remember how we talked about updating the embedding of the edges and the nodes, what also matters is the order in which these are done. These design decisions among others(number of nodes, degree of each node etc.) are a bunch of factors that go into making efficient Graph Neural Networks.</p> <p>But wait Om, you haven’t yet touched upon the fact of how predictions are made and what on earth are these Graph Neural Networks doing with this message passing yada yada yada….</p> <p>Alright so let’s consider the following task: we have a bunch of drug molecules and their effectiveness listed against a disease(umm…say tuberculosis). What we can try to do is to have a model that could predict the effectiveness of newer molecules tuberculosis. So in order to leverage the power of GNNs, we would ideally need to express our molecules as graphs first and then pass them through the network.</p> <p>So consider the following molecule:</p> <div class="row mt-3" style="max-width: 1000px; margin: 0 auto;"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_4.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_4.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Source: Me 🤡
</code></pre></div></div> <p>The way we could go about representing this as a graph can vary, we can either consider individual atoms as nodes or some smaller fragments of the molecule as nodes, in which we would need to specify the index of the node that other fragments can connect to. So let’s say we take a CO fragment from the molecule the vector representing the positions where you can connect to can look like <a href="since you can attach two more atoms to the carbonyl carbon">0,0</a> and you can maybe have a feature vector that represents some additional information about the particular molecule. Need an example for that too??? Well C’mon &gt;_&lt;… okay okay I will give you one…</p> <div class="row mt-3" style="max-width: 400px; margin: 0 auto;"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_5.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_5.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Source: Me 🤡
</code></pre></div></div> <p>Consider you choose to represent features like toxicity, pH, polarity etc.. about every fragment/atom/node you have with you. So you will create a vector, that looks something like [0.5,0.4,0.9,…..] with the numbers in the vector representing those particular features about the fragment. This is the embedding….that I have been screaming about throughout this post. You can maybe try having a similar feature vector for the bonds as well, then comes in the pooling operation where you do crazy shit(nah just gather all the vectors together, put them into a matrix after applying either sum or mean function). Then pass this and the other nodes through the neural network to learn the relation between each fragment and how they affect the molecules’ effectiveness against the particular disease. So in the end output of our model is single number which could either be MIC(how well it inhibits a bacteria) or some other metric.</p> <p>After which you have the good old backpropagation to learn the feature vectors. Now of course there are other tricks up our sleeves which we can leverage to make these predictions better, like also utilizing the information about the entire graphs and the connections rather than just it’s nodes and edges, and also what types of graphs we choose to represent the data matters. But as we have seen, Graphs in general can be complicated sometimes, and life I feel is nothing but a graph, and it’s best traversed when we do it one node at a time(❁´◡`❁)</p> <p>PS: If you find any errors in the above writing, I beg you to shatter my delusions at okhere21@gmail.com</p>]]></content><author><name></name></author><category term="Posts"/><category term="GNNs"/><summary type="html"><![CDATA[A brief intro to Graph Neural Networks]]></summary></entry><entry><title type="html">Track me if you can!</title><link href="https://w-ok-e.github.io/blog/2024/balltracking/" rel="alternate" type="text/html" title="Track me if you can!"/><published>2024-10-01T18:00:00+00:00</published><updated>2024-10-01T18:00:00+00:00</updated><id>https://w-ok-e.github.io/blog/2024/balltracking</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2024/balltracking/"><![CDATA[<p>How hard is it for you to keep track of the tennis ball when Federer and Nadal are up against each other? Quite easy, no? Barring of course, the torture that your neck has to undergo.</p> <video width="640" height="360" autoplay="" loop="" muted="" controls=""> <source src="giphy.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> <p>But then, I thought to myself, how hard could this be for a machine? And how would they exactly go about achieving the task? So we will be taking up this seemingly simple task today - to detect or track a volleyball and find the number of players in a volleyball match, but using Computer Vision!</p> <p>Getting the Ball in your Court</p> <p>To begin with, let us first look at how do computers process image data. So an image for Computers is nothing but a set of numbers in a matrix, which essentially represent pixel values. So for instance this….</p> <p>Source</p> <p>is a visual depiction of how your lappy looks at the number ‘8’. Each tiny value that you see represents how bright or dark that particular pixel is…and here the values range from 0(black) -255(white), now similarly for color images, we can have three values(RGB or HSV, depending upon what color model you choose) and so on.</p> <p>But here we are tasked with tracking a ball in a video right? So why talk about images? Well, what is a video if not a bunch of images being displayed in quick succession? Hell, I even used to think that there’s someone sitting behind the T.V. screen doing all the flipping for us, good to know I was kinda right ;) So now, the rate at which these images are being flashed across the screen is determined by something called “fps” i.e. Frames Per Second, which basically decides how many “frames” or “pictures” do we flash to create an illusion of motion.</p> <p>Now, the Human eye can be fooled at about 10-12 fps, hence most of the videos that we see out there are 25 fps(not to confuse with terms like 720p or 1080p which measure the number of pixels that there are in each frame) while movie cameras can shoot at about 40 fps. So yeah, a video is essentially a bunch of images being flashed across the screen so quickly, that the human eye is fooled and perceives it as a video.</p> <p>And that’s how we are proceeding with the given task. We are reading the video frame by frame, doing a bunch of operations on them, and figuring out where the ball is in that frame. We do that for all the frames and viola! We track the ball successfully. Now of course, there are “tracking algorithms” that can predict the position of the ball in the next frame and track it, but let’s keep that for a future post shall we.</p> <p>The Ball’s story before it is tracked</p> <p>So for this we have chosen quite an interesting video, have a look:</p> <p>source: author</p> <p>Now, you see the interesting or rather the tricky thing here is that not only is the Ball yellow in color, the Brazilian players too are in yellows, which means simply applying color filtering won’t cut it.</p> <p>Gaussian Blurring</p> <p>Why do we need to Blur the frames? Well, when we think of blurring, we mostly think of the loss of detail and information. But sometimes, too much detail is harmful too. Here in this case, where the algorithm is looking for a ball, the sharp detail of the players, or the court can often confuse and mess with the distinction. Hence blurring the image helps reduce the details and makes it easier to detect the object/region of interest.</p> <p>But what on earth is “Gaussian Blurring” then?</p> <p>So it starts by choosing a matrix or in fancy jargon - a “Kernel” with odd but equal number of rows &amp; columns, with each value in the kernel being a ‘weight’. So to think about it, the size needs to be odd because we need a central value in the kernel, and it is not possible to have one with a kernel of even size. Now each value in white that you see in each box are the ‘weights’ associated with it or the emphasis that the pixel value in that box will be given.</p> <p>Hence when our kernel is in the top left corner of the image, the value that the central pixel of the resultant image takes in the blurred image is a weighted average of all the pixels that the Kernel covers, and this weighted average is put in place of the central pixel, that is why we need to have a square matrix with odd shape and a central value. Pheww…so much Jargon !!</p> <p>Look at it through a different lens!</p> <p>Alright, there’s one detail that is really crucial here. There is a catch here to the way we are reading the frames from the videos. We briefly mentioned the RGB color model above right, it is the most common kind of color model that is used for images that you see, so we mix various intensities of three base colors - Red, Green and Blue to obtain a subset of the colors in the visible spectrum. Yeah you read it right, it’s impossible to obtain all the colors using certain RGB models(there are variations there too, but we won’t go there)</p> <p>Now the catch here is that, after we have read the frame from the video, we convert it from RGB to HSV(more on it soon). But the reason for switching to HSV is not the limitations faced by RGB in representing every color in the spectrum, rather it is in the way the RGB model responds to variations.</p> <p>HSV or Hue, Saturation, Value is has three parameters or numbers, just like RGB but they have different characteristics. Hue for instance, handles the color part and ranges from 0 to 360 degrees. So for instance shades of Red fall between 0 and 60 degrees, yellow from 61-120 degrees and so on.</p> <p>Saturation talks about the “amount of gray” or “amount of color” in the image. And ranges from 0-100 percent, where 0 means more gray, and towards 100 means more of the color.</p> <p>Value handles the brightness/luminosity of the image and just as it’s brother Saturation, ranges from 0-100 percent.</p> <p>Now the reason why we are going with HSV over RGB or some other color model for the purpose is that there are very less variations from color to color in the HSV range than in RGB. To think of it, two shades of red will be closer in the HSV color model than in RGB. So imagine if in our video, the ball comes under a light source and is brighter than usual for a moment, if our image is in HSV, the range that we have asked the computer to look for won’t change much, as the two shades are closer on the HSV scale, but at the same time, if the model used were RGB, the resultant change will be significant and our pre-defined range could be rendered useless.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                                                 The HSV color model [source] 
</code></pre></div></div> <p>As they say: Look at what’s relevant</p> <p>So we define a range of color values that the machine needs to look for, what is left now is actually looking for the values. That’s easy peasy lemon squeezy! We just check whether the pixels that we are looking at, fall into the range that we defined earlier. If yes, then we replace those pixels with 255(white) or else we replace them with 0(black), so for instance, when looking for the Brazilian men in yellow in a particular frame, the “relevant look” would be something like this:</p> <p>Now obviously the outlining is nowhere close to perfect, but we only care about the overall picture here, which is more than enough to track the tiny dot in the left corner and count the number of players.</p> <p>Bustin’ the Ball</p> <p>Notice in the above binary image, we have several white blobs. How do you say which is the ball and which are the players? We would mostly make intuitive guesses here as to what their identity is, but that’s where it’s methodical and little easier although not definite, for a machine.</p> <p>We take the binary image and calculate something called contours. Contours are pretty simple, generally they are just continuous lines that connect points with the same elevation. Here in our context, these are lines that connect all the continuous points along a boundary with the same intensity.</p> <p>How does that help you ask? Well, post contour drawing, there are two ways that I have used them:</p> <p>Whatever the contour, we estimate the circle with the smallest radius that can be drawn around that, then if the radius is in a certain range, we classify them as players or else as the ball. So as expected, most of the time the circles drawn around the player contours are larger than those drawn around the ball’s contours.</p> <p>Alright the first method works fine, but there was something else too which I found helpful. Drawing bounding circles around the contours is fine, but what if we can extrapolate the shape of the object just from it’s contours? It would work like magic no? Since the ball is round, and the players are all zigggy-zaggy, once we are able to figure out the exact shape of the contours, it would be cake-walk to distinguish them.</p> <p>In the end, I applied a mixture of both the above techniques, so I confirmed that the radius of the bounding circle around a contour lay in a certain range, and also that the contours roughly represent the ball, and that is how ladies and gentlemen we manage to track the infamous illusive ball:</p> <p>source: author</p> <p>PS: The methodology used here is really basic, and the results are borderline satisfactory, I’m all ears for better ideas. Feel free to connect and collaborate!</p>]]></content><author><name></name></author><category term="Posts"/><category term="Computer"/><category term="Vision,"/><category term="OpenCV"/><summary type="html"><![CDATA[Simple Ball Tracking using hsv]]></summary></entry></feed>