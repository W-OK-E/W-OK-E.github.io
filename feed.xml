<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://w-ok-e.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://w-ok-e.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-11T14:24:30+00:00</updated><id>https://w-ok-e.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Pictures of the Unseen…</title><link href="https://w-ok-e.github.io/blog/2025/difix/" rel="alternate" type="text/html" title="Pictures of the Unseen…"/><published>2025-08-10T18:00:00+00:00</published><updated>2025-08-10T18:00:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/difix</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/difix/"><![CDATA[<p>I have been recently pulled into the world of 3D reconstrucion while I was interning this summer at an awesome lab, and trust me the wild possibilities that exist in this space are just out of this world. I am huge fans of game engines and various high fidelity animations, but what was even more exhilirating was to be able to understand some of the basic principles and concepts behind these techniques, and then utilise them to bring 2d images to life. In this blog, we will be discussing about the various methodologies that I used for 3D reconstruction, and also an exciting aspect that we have taken up recently. So let’s get into it.</p> <h2 id="the-great-gauss-to-the-rescue">The Great Gauss to the rescue</h2> <p>What are the ways in which you can think of representing 2D images in 3D? PointClouds, or maybe structural sheets wrapped around a rough skeleton of the model. Now we will discuss pointclouds and meshes later in the blog, but first I wanted to talk about something very unique and awe-striking - <strong>Gaussian Splatting</strong>. Now I don’t about you, but having seen the number of places where the name of Carl Fredrick Gauss pops up, just blows my mind. No seriously, you should have a look at it, that dude is awesome.</p> <p>Okay let’s understand the basic idea first - basically what you have is multiple 3D blobs which are either initialized randomly or based on a sparse reconstruction of the final structure from the images we have(there is a step prior to this called SFM which I’m glossing over here, but that basically involves obtaining camera poses and matching image features from a set of given images.)</p> <p>These blobs or Gaussians(hail Gauss) too have some specific properties of “volumetric radiance fields”, which basically means that the gaussian blobs we have, are capable of modelling what colors will appear and what brightness when we look at it from certain angles for any point in 3D space(which if you know, sounds very similar to a NERF or Neural Radiance Fields concept-wise but they’re different in the way structures are represented), specifically they look something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/gs_nerf-480.webp 480w,/assets/img/post_ims/difix/gs_nerf-800.webp 800w,/assets/img/post_ims/difix/gs_nerf-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/gs_nerf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://medium.com/data-science/a-comprehensive-overview-of-gaussian-splatting-e7d570081362">src1</a></p> <p>But the real magic of Gaussian Splats is that they donot fall under the enraging umbrella of Neural Networks at runtime and hence beautifully illustrates how an algorithm that relies on classical methods and mathematics can still deliver great products. Although just to clarify, it does use backpropagation to optimize the parameters of the various gaussians that are initialized. So in brief once you have a sparse reconstruction of images, you take those 3d points as initial means for the Gaussians all of which have their own specific properties. These properties are then optimized via backpropagation.</p> <p>During rendering, we need to iterate through each and every pixel in the H x W image because each of them acted as a mean for the gaussian that was initialized and optimized. But again, it is important to note that during this rendering, this doesn’t pass through an MLP or a Neural Network unlike NERFs, which makes Gaussian Splatting a little faster during rendering.</p> <h2 id="why-make-a-mess-mesh">Why make a <del>Mess</del> Mesh?</h2> <p>Okay so it might have felt like blind worship of Gaussian splat in the last section(and it was because I am a huge fan of it) but Gaussian splatting also produces a lot of artifacts in the recosntruction. You can think of as when some blobs or gaussians are too elongted, and can appear spread over the place. Meshes on the other hand, can be imagined like a cloth wrapping around a skeleton structure and taking up the structural features of the model in question. This “cloth” is actually an oversimplification of many different elements that could be connected to each other in either regular or irregular patterns. Meshes although great at representing smooth features and edges, struggle with representing complex lightning conditions, while at the same time, even though gaussian splats can represent intricate details and various lightning conditions, they struggle with stray gaussians, especially at edges and in smooth areas. The following image really delivers the point of a mesh being a cloth stitched from various smaller subcomponents.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/cat_mesh-480.webp 480w,/assets/img/post_ims/difix/cat_mesh-800.webp 800w,/assets/img/post_ims/difix/cat_mesh-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/cat_mesh.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.artstation.com/marketplace/p/Ky67/cartoon-cat-base-mesh-3d-model">src</a></p> <h3 id="when-they-borrow-the-invisibility-cloak">When they borrow the invisibility cloak…</h3> <p>Now obviously there are many more methods for 3D reconstruction and still more variants of the same, and I could keep talking about them forever but this blog is about the missing pieces. Regardless of the methodology at play, one should not forget that 3D not just derives but relies on 2D data. If we donot have the snapshots of a particular scene/object from certain angles, it becomes very hard to model them. In case of Gaussian Splats,if we donot have ground truth captures for a model from certain angles, the mean points for gaussians in that particular region will be missing, which means that the gaussians surrounding that area will now try to fill that region up, which either means artifacts in the final reconstruction or poor splat generation.</p> <p>Here is a snapshot of a 3D reconstruction of a temple in Orissa - Somnath, which was done using fewer images than required:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/splat_decent-480.webp 480w,/assets/img/post_ims/difix/splat_decent-800.webp 800w,/assets/img/post_ims/difix/splat_decent-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/splat_decent.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>[source - Image from Author]</p> <h3 id="diffusing-it-away">Diffusing it away</h3> <p>Okay so let us first round out the problem that we have here - We are missing snapshots from certain angles or poses, and hence the view generated from is full of artifacts. What if, there was a way to take a snapshot of that view with artifacts and also obtain the camera pose along with it, because remember - for most 3D reconstrucion pipelines, we don’t just need the images, we also need the camera poses, so we also need a way to sort of first obtain the novel camera position. The approach we are gonna follow will be something like:</p> <ul> <li>Given the training or the ground truth camera positions already present in out dataset, we can interpolate to find camera positions that are not already there.</li> <li>From the novel camera poses that we obtain, we would then need a way to sort take a snapshot to get the novel view which might contain artifacts from the surrounding gaussians</li> <li>Once we have this novel view, we need to think of a way to get rid of these artifacts, and obtaining something closer to the ground truth view.</li> </ul> <p>Now I don’t about you, but in hindsight, to me this feels like an elegant idea that sounds so trivial. This is partly inspired from a few research papers I came across and also inputs from my professor and mentors.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/camera_poses-480.webp 480w,/assets/img/post_ims/difix/camera_poses-800.webp 800w,/assets/img/post_ims/difix/camera_poses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/camera_poses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Visualizing Camera Poses for a 3D model" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>[source - Image from Author]</p> <p>So having established the idea, let us see the ways in which we can attain the aforesaid goals. Now interpolating and taking snapshots can only happen once we have rendered the model as a gausian splat. And for the rendering part, we have NerfStudio, and using the python API we can even automate the process headless.</p> <p>We can interpolate from the existing camera positions and then rasterize the splats to obtain the snapshot, so that we end up with additional camera positions and their corresponding novel views, which can be used downstream for training. But there’s a catch, these images that we have obtained have artifacts so we need to find a way of fixing that.</p> <h3 id="controlled-diffusion">Controlled Diffusion</h3> <p>I am pretty sure you must have experimented with Ghibli art style when the trend was hot, but no we aren’t gonna talk about it today. We are gonna take up it’s predecessor - Stable Diffusion. Diffusion is capable of generating realistic images just from noise, essentially it learns that mapping of being able to “diffuse” noise in just the right amounts to end up with an image. But then if you have ever used any of those awesome demos online, you would have noticed that “A happy Koala made out of Blueberry Cake” doesn’t always yield the same image, there are variations to every generation, unless ofcourse we make the models deterministic by fixing the random samplers and the seed.</p> <p>And by now you must have guessed it, that if we plan to use a diffusion model to “fix” the artifacts or say “diffuse” them away predictively from our novel views, we will need some way of controlling the process. That is where another one of Deep Learning models come in - <strong>ControlNets</strong>!</p> <p><strong>ControlNets</strong> essentially aim at more informed or controlled image generation from diffusion models. Imagine your cat posed really beautifully the other day, and now you’re craving some more of the same 🐈. And ofcourse the mean creatures that they are, your cat is refusing to recreate the master-piece, add to that the utter difficulty it has been to explain the pose to a generative model. That’s where ControlNets come in handy, you can pass the image of your cat along with some other variants of it like Canny Edge, Depth etc. and then “condition” your model to then generate a cat in that particular pose.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/Control.webp" sizes="95vw"/> <img src="/assets/img/post_ims/difix/Control.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Visualizing Camera Poses for a 3D model" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://generativeai.pub/how-to-setup-controlnet-for-stable-diffusion-ai-step-by-step-guide-aafff8996719">src</a></p> <p>This miraculously allows us to condition diffusion models to “fix” the image containing artifacts by passing the nearest ground truth image, their depth maps(denoting the distance from the camera), the confidence map(what is the confidence score of each gaussian in the captured snapshot) along with the view that we need to fix, and viola, the model should fix this, and even though right now we are still working on this, I am super excited about this line of research, because just being able to spin models without even rendering them and them fixing those “broken” views via “controlled” generation just blows my mind, what about you?</p>]]></content><author><name></name></author><category term="Posts"/><category term="Diffusion,"/><category term="Deep"/><category term="Learning,"/><category term="Computer"/><category term="Vision,"/><category term="Research"/><summary type="html"><![CDATA[A short description of how we aim to improve 3D reconstruction using Diffusion Priors.]]></summary></entry><entry><title type="html">Is Cancer Modern or Retro &amp;gt;_&amp;lt; ?</title><link href="https://w-ok-e.github.io/blog/2025/cancer/" rel="alternate" type="text/html" title="Is Cancer Modern or Retro &amp;gt;_&amp;lt; ?"/><published>2025-08-03T14:24:00+00:00</published><updated>2025-08-03T14:24:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/cancer</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/cancer/"><![CDATA[<p>We have all known for quite some time now that Cancer is caused by mutations in the Human DNA which results in uncontrolled proliferation of the cell, resulting in tumors which sort of disrupt every function that the body is capable of. But believe you me, it was not always this simple. We still do not understand the exact mechanism behind this menace which is one of the reasons why it is so hard to come up with a cure. But today’s tale is not about the technicalities, rather it is about how once the research community was divided between viral and the genetic possibilities as potential causes for Cancer.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/cancer1/virus_vs_dna-480.webp 480w,/assets/img/post_ims/cancer1/virus_vs_dna-800.webp 800w,/assets/img/post_ims/cancer1/virus_vs_dna-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/cancer1/virus_vs_dna.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.dreamstime.com/illustration/dna-cartoon.html">src1</a> <a href="https://www.istockphoto.com/photos/coronavirus-animation">src2</a></p> <h2 id="but-why-look-at-viruses-at-all">But why look at Viruses at all?</h2> <p>If Alexander Fleming and other scientists had never managed to reproduce penicillin in subsequent experiments and tests once it appeared accidently on the petri dishes, society might still be plagued by even the simplest of ailments. Simply put, in order to better understand something or make use of the discovery, it must be easily reproducible in the laboratory, and in the late 1960s this was not the case with Cancer. Although the obvious culprits had been rounded out by then - Tobacco, asbestos, X-Rays, but I think that even you would agree that it is not the most scientific or the best method to induce cancer in a cell/subject by exposing it to lethal doses of X-Rays, or making a person smoke 20 cigarattes a day. They needed a reiable way of inducing caner in cells which can then be studied further.</p> <p>But there was one such candidate - <strong>Rous’s Sarcoma Virus</strong> was a rare virus that caused a rare cancer in a species of Chicken. And it’s namesake Peyton Rous, was already pushing Viruses as the key missing piece in the puzzle of Cancer’s causes. But little did he realise that this push would become the missing piece to a different puzzle than Cancer Viriology. But RSV came with it’s own caveats, the virus had never been isolated on a petri-dish to reliably study it. Until One day…</p> <h3 id="cancer-in-a-petri-dish">Cancer in a Petri-Dish:</h3> <p>Howard Temin joined Renato Dulbecco’s laboratory at Caltech in 1951 where he started his work in understanding the genetics of Fruit Flies, but soon shifted his focus to RSV with the aim of creating cancer in a petri-dish, and did indeed manage to do so. He was able to infect a few cells in a petri-dish with RSV and incite them to multiply and grow uncontrollably. This was groudnbreaking, if you did not realize it yet, this means that everything that is needed for the malignant process of cancer to begin and do it’s dirty work, all of it is present inside the cell itself.</p> <h4 id="be-my-host">Be my host</h4> <p>Having separated the devil from the host, Temin could now study cancer in ways that had not been thought possible, and boy was he in for a surprise… As any basic biology book would tell you, when a virus enters it’s host, it infects the cell, produce more viruses and then infect more cells. Nowhere in the process is the cell’s genetic makeup altered. But RSV behaved differently, the virus had physically ‘attached’ itself to the cell’s DNA structure, causing it to multiply uncontrollably, which was unprecendted.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/cancer1/dna_alter-480.webp 480w,/assets/img/post_ims/cancer1/dna_alter-800.webp 800w,/assets/img/post_ims/cancer1/dna_alter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post_ims/cancer1/dna_alter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://www.zeclinics.com/blog/what-is-gene-editing-and-how-does-it-work/">src1</a></p> <p>The reason this was both surprising and confusing to Temin and his companions was that like many viruses RSV was also known to carry it’s genetic makeup in the form of RNA. Usually, it is this RNA that directly translates into viral proteins when a virus infects a cell and result in several copies of the virus from within the cell itself WITHOUT making any changes to the DNA makeup of the host cell. So how could a copy of the RSV’s genes convert to DNA, it was a well established fact in Cell Biology at that time that this was impossible.</p> <p>But just as every groundbreaking discovery, this hypothesis from Temin would need some solid backing and he had none at the beginning. His was the job now to isolate the enzyme and obtain the evidence for this reverse flow of information from RNA to DNA. As interesting as the detailed story is, we wouldn’t dive into it here, but he and his colleagues did manage to isolate the enzyme responsible for this particular effect which was Temin’s evidence, evidence that RSV was no ordinary virus, it was infact a <em>Retrovirus</em>.</p> <h3 id="the-wall-of-silence">The Wall of Silence</h3> <p>Now the discovery made my Temin and a contemporary researcher was ground-breaking, and two reports containing very similar findings were published in Nature Magazine in 1970, and it was immediately hailed by Cancer Researchers as the default mechanism behind cancer. But this seemed to have no effect whatsoever on the cancer oncologists, and very little changed in how Cancer was treated and dealt with, even in the minds of people like Sidney Farber(the father of modern chemotherapy) who had attended the conference, this discovery offered little to no help in how Cancer could be treated.</p> <h3 id="the-bubble">The bubble</h3> <p>By this time, there were two prevalent theories on the cause of cancer - Exogenous and Endogenous. The Exogenous theory was basically the viral theory of cancer i.e. Cancer was caused by viruses, while the Endogenous theory looked inwards - at genes and the DNA as a possible causal agent for cancer. Temin’s discovery hinted that an RNA virus could enter a cell, make a DNA copy of it’s genes, and then attach that to the cell’s genome. This led one virologist - Sol Spiegelman to conjure an interesting theory.</p> <p>Spiegelman suggested that over multiple generations these viral genes may get incorporated into a host’s genome and then get activated by a then unknown mechanism, and cause the cell to multiply uncontrollably. This theory was so tantalizing that Spiegelman set out to find retroviruses in almost all forms of cancer and more often than not, he ended up finding them. As more and more funding poured into this supposedly ground-breaking venture, Spiegelman managed to find more and more retroviruses which triggered even more funding, finally - people thought, a definite cause and hence a cure for the beast was within reach.</p> <p>When labs all aroudn the U.S. tried to replicate Spiegelman’s findings, it was realized that in the frenzy of finding retroviruses and having already established them as a definite cause for cancer in his mind, Speigelman had found retroviruses in places they did not exist. All the money could not make Speigelman’s findings fly, but the discovery of retroviruses and the frenzied studies on them were definitely not for nought, as Spiegelman would take his last breathes, they would in the wake of a strange illness among gay men and people with blood transfusions, and an year after Spiegelman passed away, it would be a retrovirus that would rise up as the causal agent for the disease - the Human Immunodeficiency Virus or <strong>HIV</strong>, but that’s an article for next Sunday.</p>]]></content><author><name></name></author><category term="Posts"/><category term="Cancer,"/><category term="Book"/><summary type="html"><![CDATA[A teensy tiny story from our everlasting battle with cancer.]]></summary></entry><entry><title type="html">The flow of Generative Networks</title><link href="https://w-ok-e.github.io/blog/2025/gnn/" rel="alternate" type="text/html" title="The flow of Generative Networks"/><published>2025-07-01T14:24:00+00:00</published><updated>2025-07-01T14:24:00+00:00</updated><id>https://w-ok-e.github.io/blog/2025/gnn</id><content type="html" xml:base="https://w-ok-e.github.io/blog/2025/gnn/"><![CDATA[<p>Recently, I have been trying to generate drug samples using generative flow architectures and by now I have gotten accustomed to my prof. nodding in utter disappointment at the samples that my models generate. Which makes me question the fact that when models like Dall-E and stable diffusion can generate such a wide variety of images, what is the bottleneck in trying to generate chemical molecules from a given sample of similar drugs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_1.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_1.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="but-what-are-generative-flow-networks-aka-gflownets">But what are “Generative Flow Networks” a.k.a. GFlowNets</h2> <p>Generative Flow Networks are quite a recent phenomena, and have been inspired from Reinforcement Learning and Deep Learning and personally for me, it was quite a task to grasp the concept behind them. Primarily because GFlowNets are not just an independent concept, but rather a mixture of a host of different Machine Learning Concepts. One of them being Graph Neural Networks. So I feel it would suffice to kinda delve into Graph Neural Networks here and maybe cover GFlowNets in another one, we’ll see.</p> <h3 id="graph-neural-networks">Graph Neural Networks:</h3> <p>It would be a very good idea to start with what a graph actually is. Now to most of us, they might seem abstract, but graphs pop up almost everywhere you can find entities(nodes) that are related among themselves(depicted by edges) via some pre-defined notion.</p> <p>Now normal graphs and networks can only represent relations to a certain extent, and we can additionally specialize them via the concept of directed and undirected edges. We have obviously read and seen graph data in context of social networks or citation networks(Scientists citing each other) but there are a few interesting places where graphs tend to yield some insightful patterns and ideas when used.</p> <p>For instance Images, now obviously using graphs to represent images sounds totally absurd and useless. Because Images have a very nice structured pattern to them, that is one reason why they are arranged in 2d or 3d arrays as bands. But picture this, what if we were to represent every pixel in the image as nodes with the adjacent pixel(maybe even of the different color channel) forming the neighbors that are connected via appropriate edges. Although very redundant, this actually paints a very nice picture if you think about it in a particular way. There is a representation for graphs that is quite commonly known as the adjacency matrix, which is a way of representing nodes of a graph and their connections.</p> <p>So say there are 25 pixels in an image, we order them in a 5 x 5 matrix and fill the entries in the matrix such that they represent the edges shared between two nodes i.e. pixels in the case of an image.</p> <p>Now it doesn’t matter whether you love math or graphs or not, If you have a soul, you have to appreciate the underlying patterns that are popping up here. But there is only so much beauty one can appreciate, because when the question of efficiency comes, this is clearly not the best choice out there.</p> <h3 id="going-beyond-beauty">Going beyond Beauty:</h3> <p>In the wild though, graphs find application in not just the most beautiful of domains, but also the useful ones. Take heterogenous data like molecules, they are the building blocks of matter with electrons and atoms hanging in 3d space joined to their brethren via bonds and that too different kinds of - Single/Double/Covalent/Ionic. It’s a very convenient and common abstraction to describe molecules then, as Graphs! With nodes representing molecules and edges representing covalent bonds.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_2.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_2.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://distill.pub/2021/gnn-intro/">source</a></p> <p>Graphs can help us make sense of the most cluttered up data like in the case of social networks, which can be very insightful in figuring out patterns and collective behavior of people and of entities that are inter-connected.</p> <p><strong>Confused?</strong></p> <p>Alright so we have discussed a few use cases where we can use graphs to represent the data but what after that? What tasks can we perform on these collections once they are represented as graphs and how?</p> <p>So let’s first take a look at the tasks that we can perform on the graphs once we have represented the required data using them, and then dive into how GNNs can make the task simpler.</p> <h4 id="graph-level-task">Graph Level Task</h4> <p>First of many tasks that can be performed are “graph level tasks” i.e. looking at the graphs as a whole and then predicting the property of the entire graph. For instance, predicting how a molecule smells like based on the graphical representation of the molecules that we have is a graph level task. Or to draw a rough analogy, a graph level task would be akin to trying to classify certain images of the dataset like CIFAR 10, while with text, a similar problem is sentiment analysis where we want to identify the mood or the context of a given token of sentence.</p> <h4 id="node-level-task">Node Level Task</h4> <p>A node level task is associated with trying to predict the properties of a node in a graph, for instance you can imagine a small social circle as a graph and then a node level task there would be to classify each node to be belonging to a certain class.</p> <h4 id="edge-level-task">Edge Level Task</h4> <p>Another important aspect that we would like to deal with is classifying the relationship between various objects present in an image. That can fall under an edge level task. Consider a image that depicts a pitcher on a baseball field. If we consider all the entities present in the image as nodes, and then represent connections between them as edges of a graph, then one of the many relations that these edges can depict is the “action” between any two objects. For instance the pitcher and the hitter can be connected as “playing”.</p> <h3 id="a-suitable-representation">A suitable representation</h3> <p>But to perform all of these awesome tasks, we would need a representation for these graphs that would let us work with them in a mathematical setting where it’s more about numbers than pictures. ‘Cause even though the enamel of your teeth is harder than steel, we don’t use it in construction(Ha Ha bad joke)</p> <p>Alright so taking a step back, we are looking for a mathematical or a computer scienc-ish representation of graphs, so then we think about what information about a graph do we need to capture - Edges and Nodes, and which nodes are connected and by which edges.</p> <p>One possibility is the good old <strong>Adjacency Matrix</strong>, so consider a example below:</p> <div class="row mt-3" style="max-width: 500px; margin: 0 auto;"> <div class="col-md-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/Graph.avif" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/Graph.avif" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-md-6 mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_3.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_3.webp" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the matrix, the entry 1 indicates a connection between the corresponding nodes in the Graph. Now although this is a very nice representation, it’s not very memory efficient. Add to that the fact that if the position of any of the nodes is switched, the matrix completely changes, so it would be like playing dice while using these matrices as inputs to a model.</p> <h4 id="adjacency-lists">Adjacency Lists</h4> <p>It’s almost as if we have the sibling of an Adjacency Matrix to help us out now but this time the memory usage is very efficient. This time we use a tuple to capture what nodes are connected, so for instance from the previous graph, the nodes 2,4 are connected, so the adjacency list would contain the tuple (2,4) in it, hence the above graph can simply be represented as :</p> <p>[[1,4],[2,4],[3,4]], now notice how even if we change the order, it doesn’t make a difference, i.e. the list [[2,4],[1,4],[3,4]] represents the same information as the former. This nice property is called “Permutation Invariance”.</p> <p>Talking about a graph level task, we need efficient transfer of data between two nodes of the graph to be able to make complex predictions.</p> <p>This technique or message-passing is the primary technique methodology behind all of the things that we plan to accomplish using a Graph Neural Network.</p> <p>Information from each node/edge is collected and then aggregated using some function before being reapplied to the whole graph area and updating the information. But this turns out to be an issue for nodes/edges that are far apart because messages take longer to transmit even though we apply message passing multiple times across those nodes.</p> <h2 id="graph-neural-networks-1">Graph Neural Networks</h2> <p>So let’s talk about Graph Neural Networks then, with all the information about the graph loaded into the adjacency list(adjacency list is better as the order doesn’t change the information expressed). We will be discussing about the simplest GNN architecture which use the method of message passing to do the required tasks.</p> <p>But what in the wild world is message passing!????</p> <p>Okay I will break it down a bit, so imagine that you treat a molecule’s structure as a graph, then the information or essentially the numbers present in the nodes could tell you something about the atoms, those in the edges could tell you something about the bonds and when taken together as an aggregate, they convey something about the whole graph or the molecule. So the idea of message passing is for a node in the graph to kinda collect these numbers(of it’s own and those of it’s neighbors), aggregate them(via some function maybe mean or sum) and then update the other nodes about the same.</p> <p>But this turns out to be an issue for nodes/edges that are far apart because messages take longer to transmit even though we apply message passing multiple times across those nodes.</p> <p><strong>Solution to the problem of message transfer</strong></p> <p>One of the possible ways in which we could tackle the issue of message passing between far away nodes, is by using global representation of a graph or something called the context vector.</p> <p>The global context vector is connected to all other nodes and edges of the network and can act as a communicator between the nodes and the edges. So you can think of it as the internet that connects that two places far apart on the globe, and allows seamless exchange of information between two parts of the graph, which allows the representation to be sort of more complete and more connected in a sense.</p> <p>Then the simple magic of Graph Neural Network is that this representation is passed through a them to learn the required representation. So the things learned could very likely be “what numbers in the nodes/edges allow me to represent a paracetamol molecule” etc.</p> <p>An important fact here to keep in mind is that the network doesn’t make any changes to the number of nodes, edges in the graph. So the amount of information required to represent the output graph is the same as that required to represent the output graph, the only difference is that the embeddings have been updated now.</p> <p>Now this seems very simple but we can also have some information missing from either the nodes or the edges sometimes, and then in that case if we are supposed to apply a neural network on the node embeddings, we need to “pool” the data. So what basically happens is every node gathers information from it’s surrounding neighbors via a process called pooling and aggregates it(yes you got it right, message passing) and then the neural network or the function is applied on the aggregated data.</p> <p>Now whether we choose to transfer data from nodes to edges or vice versa is something that needs to be looked into because the two embeddings need not necessarily be of the same size, so it is not very obvious as to how to directly combine them. We can again use a neural network to map from one embedding to the other or maybe concatenate multiple embeddings together(Don’t worry, think of embedding as simply a vector or even simpler a collection of numbers that represent something about the associated entity). What’s also important is the way in which information is updated. Remember how we talked about updating the embedding of the edges and the nodes, what also matters is the order in which these are done. These design decisions among others(number of nodes, degree of each node etc.) are a bunch of factors that go into making efficient Graph Neural Networks.</p> <p>But wait Om, you haven’t yet touched upon the fact of how predictions are made and what on earth are these Graph Neural Networks doing with this message passing yada yada yada….</p> <p>Alright so let’s consider the following task: we have a bunch of drug molecules and their effectiveness listed against a disease(umm…say tuberculosis). What we can try to do is to have a model that could predict the effectiveness of newer molecules tuberculosis. So in order to leverage the power of GNNs, we would ideally need to express our molecules as graphs first and then pass them through the network.</p> <p>So consider the following molecule:</p> <div class="row mt-3" style="max-width: 1000px; margin: 0 auto;"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_4.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_4.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Source: Me 🤡
</code></pre></div></div> <p>The way we could go about representing this as a graph can vary, we can either consider individual atoms as nodes or some smaller fragments of the molecule as nodes, in which we would need to specify the index of the node that other fragments can connect to. So let’s say we take a CO fragment from the molecule the vector representing the positions where you can connect to can look like <a href="since you can attach two more atoms to the carbonyl carbon">0,0</a> and you can maybe have a feature vector that represents some additional information about the particular molecule. Need an example for that too??? Well C’mon &gt;_&lt;… okay okay I will give you one…</p> <div class="row mt-3" style="max-width: 400px; margin: 0 auto;"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/GNNs/gen_net_5.webp" sizes="95vw"/> <img src="/assets/img/post_ims/GNNs/gen_net_5.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Source: Me 🤡
</code></pre></div></div> <p>Consider you choose to represent features like toxicity, pH, polarity etc.. about every fragment/atom/node you have with you. So you will create a vector, that looks something like [0.5,0.4,0.9,…..] with the numbers in the vector representing those particular features about the fragment. This is the embedding….that I have been screaming about throughout this post. You can maybe try having a similar feature vector for the bonds as well, then comes in the pooling operation where you do crazy shit(nah just gather all the vectors together, put them into a matrix after applying either sum or mean function). Then pass this and the other nodes through the neural network to learn the relation between each fragment and how they affect the molecules’ effectiveness against the particular disease. So in the end output of our model is single number which could either be MIC(how well it inhibits a bacteria) or some other metric.</p> <p>After which you have the good old backpropagation to learn the feature vectors. Now of course there are other tricks up our sleeves which we can leverage to make these predictions better, like also utilizing the information about the entire graphs and the connections rather than just it’s nodes and edges, and also what types of graphs we choose to represent the data matters. But as we have seen, Graphs in general can be complicated sometimes, and life I feel is nothing but a graph, and it’s best traversed when we do it one node at a time(❁´◡`❁)</p> <p>PS: If you find any errors in the above writing, I beg you to shatter my delusions at okhere21@gmail.com</p>]]></content><author><name></name></author><category term="Posts"/><category term="GNNs"/><summary type="html"><![CDATA[A brief intro to Graph Neural Networks]]></summary></entry></feed>