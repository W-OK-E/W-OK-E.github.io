<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pictures of the Unseen... | Om Kumar </title> <meta name="author" content="Om Kumar"> <meta name="description" content="A short description of how we aim to improve 3D reconstruction using Diffusion Priors."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/DinoTim.jpg?fb78e70fb7a3591c22a9d7e7902c88a1"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://w-ok-e.github.io/blog/2025/difix/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Om</span> Kumar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">bookshelf </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pictures of the Unseen...</h1> <p class="post-meta"> Created on August 10, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> ¬† ¬∑ ¬† <a href="/blog/tag/diffusion"> <i class="fa-solid fa-hashtag fa-sm"></i> Diffusion,</a> ¬† <a href="/blog/tag/deep"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep</a> ¬† <a href="/blog/tag/learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Learning,</a> ¬† <a href="/blog/tag/computer"> <i class="fa-solid fa-hashtag fa-sm"></i> Computer</a> ¬† <a href="/blog/tag/vision"> <i class="fa-solid fa-hashtag fa-sm"></i> Vision,</a> ¬† <a href="/blog/tag/research"> <i class="fa-solid fa-hashtag fa-sm"></i> Research</a> ¬† ¬∑ ¬† <a href="/blog/category/posts"> <i class="fa-solid fa-tag fa-sm"></i> Posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I have been recently pulled into the world of 3D reconstrucion while I was interning this summer at an awesome lab, and trust me the wild possibilities that exist in this space are just out of this world. I am huge fans of game engines and various high fidelity animations, but what was even more exhilirating was to be able to understand some of the basic principles and concepts behind these techniques, and then utilise them to bring 2d images to life. In this blog, we will be discussing about the various methodologies that I used for 3D reconstruction, and also an exciting aspect that we have taken up recently. So let‚Äôs get into it.</p> <h2 id="the-great-gauss-to-the-rescue">The Great Gauss to the rescue</h2> <p>What are the ways in which you can think of representing 2D images in 3D? PointClouds, or maybe structural sheets wrapped around a rough skeleton of the model. Now we will discuss pointclouds and meshes later in the blog, but first I wanted to talk about something very unique and awe-striking - <strong>Gaussian Splatting</strong>. Now I don‚Äôt about you, but having seen the number of places where the name of Carl Fredrick Gauss pops up, just blows my mind. No seriously, you should have a look at it, that dude is awesome.</p> <p>Okay let‚Äôs understand the basic idea first - basically what you have is multiple 3D blobs which are either initialized randomly or based on a sparse reconstruction of the final structure from the images we have(there is a step prior to this called SFM which I‚Äôm glossing over here, but that basically involves obtaining camera poses and matching image features from a set of given images.)</p> <p>These blobs or Gaussians(hail Gauss) too have some specific properties of ‚Äúvolumetric radiance fields‚Äù, which basically means that the gaussian blobs we have, are capable of modelling what colors will appear and what brightness when we look at it from certain angles for any point in 3D space(which if you know, sounds very similar to a NERF or Neural Radiance Fields concept-wise but they‚Äôre different in the way structures are represented), specifically they look something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/gs_nerf-480.webp 480w,/assets/img/post_ims/difix/gs_nerf-800.webp 800w,/assets/img/post_ims/difix/gs_nerf-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post_ims/difix/gs_nerf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><a href="https://medium.com/data-science/a-comprehensive-overview-of-gaussian-splatting-e7d570081362" rel="external nofollow noopener" target="_blank">src1</a></p> <p>But the real magic of Gaussian Splats is that they donot fall under the enraging umbrella of Neural Networks at runtime and hence beautifully illustrates how an algorithm that relies on classical methods and mathematics can still deliver great products. Although just to clarify, it does use backpropagation to optimize the parameters of the various gaussians that are initialized. So in brief once you have a sparse reconstruction of images, you take those 3d points as initial means for the Gaussians all of which have their own specific properties. These properties are then optimized via backpropagation.</p> <p>During rendering, we need to iterate through each and every pixel in the H x W image because each of them acted as a mean for the gaussian that was initialized and optimized. But again, it is important to note that during this rendering, this doesn‚Äôt pass through an MLP or a Neural Network unlike NERFs, which makes Gaussian Splatting a little faster during rendering.</p> <h2 id="why-make-a-mess-mesh">Why make a <del>Mess</del> Mesh?</h2> <p>Okay so it might have felt like blind worship of Gaussian splat in the last section(and it was because I am a huge fan of it) but Gaussian splatting also produces a lot of artifacts in the recosntruction. You can think of as when some blobs or gaussians are too elongted, and can appear spread over the place. Meshes on the other hand, can be imagined like a cloth wrapping around a skeleton structure and taking up the structural features of the model in question. This ‚Äúcloth‚Äù is actually an oversimplification of many different elements that could be connected to each other in either regular or irregular patterns. Meshes although great at representing smooth features and edges, struggle with representing complex lightning conditions, while at the same time, even though gaussian splats can represent intricate details and various lightning conditions, they struggle with stray gaussians, especially at edges and in smooth areas. The following image really delivers the point of a mesh being a cloth stitched from various smaller subcomponents.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/cat_mesh-480.webp 480w,/assets/img/post_ims/difix/cat_mesh-800.webp 800w,/assets/img/post_ims/difix/cat_mesh-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post_ims/difix/cat_mesh.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><a href="https://www.artstation.com/marketplace/p/Ky67/cartoon-cat-base-mesh-3d-model" rel="external nofollow noopener" target="_blank">src</a></p> <h3 id="when-they-borrow-the-invisibility-cloak">When they borrow the invisibility cloak‚Ä¶</h3> <p>Now obviously there are many more methods for 3D reconstruction and still more variants of the same, and I could keep talking about them forever but this blog is about the missing pieces. Regardless of the methodology at play, one should not forget that 3D not just derives but relies on 2D data. If we donot have the snapshots of a particular scene/object from certain angles, it becomes very hard to model them. In case of Gaussian Splats,if we donot have ground truth captures for a model from certain angles, the mean points for gaussians in that particular region will be missing, which means that the gaussians surrounding that area will now try to fill that region up, which either means artifacts in the final reconstruction or poor splat generation.</p> <p>Here is a snapshot of a 3D reconstruction of a temple in Orissa - Somnath, which was done using fewer images than required:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/splat_decent-480.webp 480w,/assets/img/post_ims/difix/splat_decent-800.webp 800w,/assets/img/post_ims/difix/splat_decent-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post_ims/difix/splat_decent.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>[source - Image from Author]</p> <h3 id="diffusing-it-away">Diffusing it away</h3> <p>Okay so let us first round out the problem that we have here - We are missing snapshots from certain angles or poses, and hence the view generated from is full of artifacts. What if, there was a way to take a snapshot of that view with artifacts and also obtain the camera pose along with it, because remember - for most 3D reconstrucion pipelines, we don‚Äôt just need the images, we also need the camera poses, so we also need a way to sort of first obtain the novel camera position. The approach we are gonna follow will be something like:</p> <ul> <li>Given the training or the ground truth camera positions already present in out dataset, we can interpolate to find camera positions that are not already there.</li> <li>From the novel camera poses that we obtain, we would then need a way to sort take a snapshot to get the novel view which might contain artifacts from the surrounding gaussians</li> <li>Once we have this novel view, we need to think of a way to get rid of these artifacts, and obtaining something closer to the ground truth view.</li> </ul> <p>Now I don‚Äôt about you, but in hindsight, to me this feels like an elegant idea that sounds so trivial. This is partly inspired from a few research papers I came across and also inputs from my professor and mentors.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/camera_poses-480.webp 480w,/assets/img/post_ims/difix/camera_poses-800.webp 800w,/assets/img/post_ims/difix/camera_poses-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post_ims/difix/camera_poses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Visualizing Camera Poses for a 3D model" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>[source - Image from Author]</p> <p>So having established the idea, let us see the ways in which we can attain the aforesaid goals. Now interpolating and taking snapshots can only happen once we have rendered the model as a gausian splat. And for the rendering part, we have NerfStudio, and using the python API we can even automate the process headless.</p> <p>We can interpolate from the existing camera positions and then rasterize the splats to obtain the snapshot, so that we end up with additional camera positions and their corresponding novel views, which can be used downstream for training. But there‚Äôs a catch, these images that we have obtained have artifacts so we need to find a way of fixing that.</p> <h3 id="controlled-diffusion">Controlled Diffusion</h3> <p>I am pretty sure you must have experimented with Ghibli art style when the trend was hot, but no we aren‚Äôt gonna talk about it today. We are gonna take up it‚Äôs predecessor - Stable Diffusion. Diffusion is capable of generating realistic images just from noise, essentially it learns that mapping of being able to ‚Äúdiffuse‚Äù noise in just the right amounts to end up with an image. But then if you have ever used any of those awesome demos online, you would have noticed that ‚ÄúA happy Koala made out of Blueberry Cake‚Äù doesn‚Äôt always yield the same image, there are variations to every generation, unless ofcourse we make the models deterministic by fixing the random samplers and the seed.</p> <p>And by now you must have guessed it, that if we plan to use a diffusion model to ‚Äúfix‚Äù the artifacts or say ‚Äúdiffuse‚Äù them away predictively from our novel views, we will need some way of controlling the process. That is where another one of Deep Learning models come in - <strong>ControlNets</strong>!</p> <p><strong>ControlNets</strong> essentially aim at more informed or controlled image generation from diffusion models. Imagine your cat posed really beautifully the other day, and now you‚Äôre craving some more of the same üêà. And ofcourse the mean creatures that they are, your cat is refusing to recreate the master-piece, add to that the utter difficulty it has been to explain the pose to a generative model. That‚Äôs where ControlNets come in handy, you can pass the image of your cat along with some other variants of it like Canny Edge, Depth etc. and then ‚Äúcondition‚Äù your model to then generate a cat in that particular pose.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_ims/difix/Control.webp" sizes="95vw"></source> <img src="/assets/img/post_ims/difix/Control.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Visualizing Camera Poses for a 3D model" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><a href="https://generativeai.pub/how-to-setup-controlnet-for-stable-diffusion-ai-step-by-step-guide-aafff8996719" rel="external nofollow noopener" target="_blank">src</a></p> <p>This miraculously allows us to condition diffusion models to ‚Äúfix‚Äù the image containing artifacts by passing the nearest ground truth image, their depth maps(denoting the distance from the camera), the confidence map(what is the confidence score of each gaussian in the captured snapshot) along with the view that we need to fix, and viola, the model should fix this, and even though right now we are still working on this, I am super excited about this line of research, because just being able to spin models without even rendering them and them fixing those ‚Äúbroken‚Äù views via ‚Äúcontrolled‚Äù generation just blows my mind, what about you?</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/quarv/">Lights‚ùå Language‚úÖ Camera...Action</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/quad/">Your dog can sniff, mine scans...</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/cancer/">Is Cancer Modern or Retro &gt;_&lt; ?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/gnn/">The flow of Generative Networks</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Om Kumar. Hosted with ‚ù§Ô∏è by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/plotly.js@3.0.1/dist/plotly.min.js" integrity="sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=" crossorigin="anonymous"></script> <script defer src="/assets/js/plotly-setup.js?5e81fc889064852664784cb29c0d6970" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>